{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP47590: Advanced Machine Learning\n",
    "# Assignment 1: Building Heterogenous Ensembles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Student 1 Name: Finola Cahill\n",
    "- Student 1 Number: 07645074"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import math\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.linear_model import LinearRegression as LR\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted, check_random_state\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn import tree\n",
    "from sklearn import svm\n",
    "from sklearn import ensemble\n",
    "from sklearn import linear_model\n",
    "from sklearn import neighbors\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import is_classifier\n",
    "from sklearn.utils import resample\n",
    "from sklearn.base import clone\n",
    "from sklearn.datasets import load_iris\n",
    "import itertools\n",
    "from itertools import chain, combinations\n",
    "import random\n",
    "from copy import copy\n",
    "from scipy import stats\n",
    "from collections import Counter\n",
    "import collections\n",
    "from sklearn.metrics import recall_score\n",
    "import matplotlib.patches as mpatches\n",
    "from statistics import mean\n",
    "from keras.utils import to_categorical\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: The Heterogenous Ensemble Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define HeterogenousEnsembleClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new classifier which is based on the sckit-learn BaseEstimator and ClassifierMixin classes\n",
    "class HeterogenousEnsembleClassifier(BaseEstimator, ClassifierMixin):\n",
    "    \n",
    "    \"\"\"An ensemble classifier that uses heterogeneous models at the base layer. Base models are different due to different hyper-parameters used.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    base_estimator: scikit-learn estimator \n",
    "        The model type to be used at the base layer of the ensemble model.\n",
    "\n",
    "    hp_range_map: dictionary\n",
    "        A dictinary of hyperparamters and the ranges of values that will be used from them\n",
    "        \n",
    "    n_estimators: int\n",
    "        How many models to use in the ensemble\n",
    "        \n",
    "    bootstrap: boolean\n",
    "        Wheter or not to use bootstrap sampling when training base estimators\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    classes_ : array of shape = [n_classes] \n",
    "        The classes labels.\n",
    "\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    The default values for most base learners are used, unless hyperparameter ranges are specified\n",
    "\n",
    "    See also\n",
    "    --------\n",
    "    \n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> from sklearn.datasets import load_iris\n",
    "    >>> from sklearn.model_selection import cross_val_score\n",
    "    >>> clf = HeterogenousEnsembleClassifier(tree.DecisionTreeClassifier(), {'max_depth':[5, 10, 15], })\n",
    "    >>> iris = load_iris()\n",
    "    >>> cross_val_score(clf, iris.data, iris.target, cv=10)\n",
    "\n",
    "    \"\"\"\n",
    "    # Constructor for the classifier object\n",
    "    def __init__(self, base_estimator = svm.SVC(), n_estimators = 10, hp_range_map = None, bootstrap = True, random_state=None, verbosity = 0):\n",
    "\n",
    "        \"\"\"Setup a SuperLearner classifier .\n",
    "        Parameters\n",
    "        ----------\n",
    "        base_estimator: The model type to be used at the base layer of the ensemble model.\n",
    "        hp_range_map: A dictinary of hyperparamters and the ranges of values that will be used from them\n",
    "        n_estimators: How many models to use in the ensemble\n",
    "        bootstrap: Wheter or not to use bootstrap sampling when training base estimators\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        The estimator\n",
    "        \"\"\"     \n",
    "\n",
    "        # Initialise ranomd state if set\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        # Initialise class variabels\n",
    "        self.base_estimator = base_estimator\n",
    "        self.hp_range_map = hp_range_map\n",
    "        self.n_estimators = n_estimators\n",
    "        self.bootstrap = bootstrap\n",
    "        self.verbosity = verbosity\n",
    "        \n",
    "    \n",
    "    def validate_parameters(self):\n",
    "        if self.n_estimators < 1:\n",
    "             raise ValueError(\"n_estimators must be >= 1\")\n",
    "        if is_classifier(self.base_estimator) is False:\n",
    "            raise ValueError(\"base_estimator must be a classifier\")\n",
    "        if self.verbosity not in range(0,3):\n",
    "            raise ValueError(\"verbosity has three levels, from 0-2\")\n",
    "        if self.hp_range_map is None:\n",
    "            self.hp_range_map = {}          \n",
    "\n",
    "    # The fit function to train a classifier\n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        self.validate_parameters()\n",
    "        \n",
    "        def checkBootstrap(X, y):\n",
    "            if self.bootstrap is True:\n",
    "                return resample(X,y, replace=True)\n",
    "            return X,y \n",
    "                \n",
    "        def checkValue(value):\n",
    "            if type(value) is np.ndarray:\n",
    "                value = value.tolist()\n",
    "            elif type(value) is not list:\n",
    "                value = [value]\n",
    "            return value + [None]\n",
    "        \n",
    "        def generate_param_combinations():\n",
    "            params = []\n",
    "            for key, value in self.hp_range_map.items():\n",
    "                params.append(checkValue(value))\n",
    "                self.keys_.append(key)\n",
    "            product = itertools.product(*params)\n",
    "            self.params_ = [ [ p for p in params ] for params in product ]\n",
    "            if self.verbosity == 2:\n",
    "                print(\"{} number of parameter combinations generated for hyperparameter range {}.\".format(len(self.params_), self.hp_range_map))\n",
    "        \n",
    "        \n",
    "        def extract_params(n):\n",
    "            params = {}\n",
    "            for i,key in enumerate(self.keys_):\n",
    "                if self.params_[n][i] is not None:\n",
    "                    params[key] = self.params_[n][i]\n",
    "            return params\n",
    "    \n",
    "        def fit_models(X, y):\n",
    "            for i in range(0, self.n_estimators):\n",
    "                X_train, y_train = checkBootstrap(X, y)\n",
    "                params = extract_params(random.randint(0, (len(self.params_)-1)))\n",
    "                clf = copy(self.base_estimator).set_params(**params)\n",
    "                if self.verbosity == 2:\n",
    "                    print(\"Fitting model {} with parameters: {}\".format(i+1, params))\n",
    "                self.models_.append(clf.fit(X_train, y_train))\n",
    "            if self.verbosity > 0:\n",
    "                print(\"{} models fitted with base estimator {}.\".format(i+1, self.base_estimator.__class__.__name__))\n",
    "        \n",
    "        # Check that X and y have correct shape\n",
    "        X, y = check_X_y(X, y)\n",
    "        \n",
    "        random.seed(self.random_state)\n",
    "        \n",
    "        self.params_, self.keys_, self.models_ = [], [], []\n",
    "        \n",
    "       #  Count the number of occurrences of each class in the target vector (uses mupy unique function that returns a list of unique values and their counts)\n",
    "        self.classes_ = np.unique(y)\n",
    "        \n",
    "        generate_param_combinations()\n",
    "        \n",
    "        fit_models(X,y)\n",
    "    \n",
    "        return self\n",
    "\n",
    "#     # The predict function to make a set of predictions for a set of query instances\n",
    "    def ensemble_predict(self, X):\n",
    "        for i, model in enumerate(self.models_):\n",
    "            self.ensemble_predictions_.append(np.array(model.predict(X)))\n",
    "            if self.verbosity == 2:\n",
    "                print('Model no. {} {} predict completed.'.format(i+1, model))\n",
    "        if self.verbosity == 1:\n",
    "            print(\"Predict completed for {} models \".format(i+1))\n",
    "        self.ensemble_predictions_ = np.asarray(self.ensemble_predictions_)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \n",
    "        self.ensemble_predictions_ = []\n",
    "                \n",
    "        def select_most_frequent(row, query_number):\n",
    "            freq, maxi = {}, 0\n",
    "            prediction, count = np.unique(row, return_counts=True)\n",
    "            for i,v in enumerate(count):\n",
    "                freq[v] = freq.get(v, []) + [prediction[i]]\n",
    "                maxi = max(maxi, v)\n",
    "            return get_mode(freq,maxi, query_number)\n",
    "        \n",
    "        def interpret_predictions():\n",
    "            if self.verbosity == 2:\n",
    "                print(\"Aggregating ensemble predictions.\")\n",
    "            final_prediction = []\n",
    "            for i,row in enumerate(self.ensemble_predictions_.transpose()):\n",
    "                final_prediction += select_most_frequent(row, i)\n",
    "            if self.verbosity == 1:\n",
    "                print(\"Results aggregated for {} queries\".format(i+1))\n",
    "            return final_prediction\n",
    "    \n",
    "        def get_mode(frequency, maxi, i):\n",
    "            if len(frequency[maxi]) > 1:\n",
    "                if self.verbosity == 2:\n",
    "                    print(\"Multiple maximums for query {}, will select at random from {}\".\n",
    "                         format(i+1, frequency[maxi]))\n",
    "                return [frequency[maxi][(random.randint(0, (len(frequency[maxi])-1)))]]\n",
    "            return frequency[maxi]\n",
    "    \n",
    "        check_is_fitted(self, ['models_'])\n",
    "        \n",
    "        # Check that the input features match the type and shape of the training features\n",
    "        X = check_array(X)\n",
    "    \n",
    "     #   self.ensemble_predictions_ = np.array([model.predict(X) for model in self.models])\n",
    "        self.ensemble_predict(X)\n",
    "        \n",
    "        return np.array(interpret_predictions())\n",
    "\n",
    "    #     # The predict function to make a set of predictions for a set of query instances\n",
    "    def predict_proba(self, X):\n",
    "        \n",
    "        self.probs_, self.ensemble_predictions_ = [], []\n",
    "\n",
    "        def calculate_probabilities():\n",
    "            if self.verbosity > 0:\n",
    "                print(\"Calculating probabilities from ensemble predictions\")\n",
    "            for p in self.ensemble_predictions_.transpose():\n",
    "                key, val = np.unique(p, return_counts=True)\n",
    "                counts = dict(zip(key, val))\n",
    "                self.probs_.append([(counts[c] / len(p)) if c in counts else 0.0 for c in self.classes_])\n",
    "            if self.verbosity > 0:\n",
    "                print(\"Probabilities calculated for {} queries and {} classes\".format(len(self.probs_), len(self.classes_)))\n",
    "        \n",
    "        check_is_fitted(self, ['models_'])\n",
    "        \n",
    "        X = check_array(X)   \n",
    "        \n",
    "        self.ensemble_predict(X)\n",
    "        \n",
    "        calculate_probabilities()\n",
    "        \n",
    "        return np.array(self.probs_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the HeterogenousEnsembleClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 models fitted with base estimator SVC.\n",
      "Predict completed for 40 models \n",
      "Calculating probabilities from ensemble predictions\n",
      "Probabilities calculated for 150 queries and 3 classes\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.   , 0.   , 0.   ],\n",
       "       [1.   , 0.   , 0.   ],\n",
       "       [1.   , 0.   , 0.   ],\n",
       "       [1.   , 0.   , 0.   ],\n",
       "       [1.   , 0.   , 0.   ],\n",
       "       [1.   , 0.   , 0.   ],\n",
       "       [1.   , 0.   , 0.   ],\n",
       "       [1.   , 0.   , 0.   ],\n",
       "       [1.   , 0.   , 0.   ],\n",
       "       [1.   , 0.   , 0.   ],\n",
       "       [1.   , 0.   , 0.   ],\n",
       "       [1.   , 0.   , 0.   ],\n",
       "       [1.   , 0.   , 0.   ],\n",
       "       [1.   , 0.   , 0.   ],\n",
       "       [1.   , 0.   , 0.   ],\n",
       "       [1.   , 0.   , 0.   ],\n",
       "       [1.   , 0.   , 0.   ],\n",
       "       [1.   , 0.   , 0.   ],\n",
       "       [1.   , 0.   , 0.   ],\n",
       "       [1.   , 0.   , 0.   ],\n",
       "       [1.   , 0.   , 0.   ],\n",
       "       [1.   , 0.   , 0.   ],\n",
       "       [1.   , 0.   , 0.   ],\n",
       "       [1.   , 0.   , 0.   ],\n",
       "       [1.   , 0.   , 0.   ],\n",
       "       [1.   , 0.   , 0.   ],\n",
       "       [1.   , 0.   , 0.   ],\n",
       "       [1.   , 0.   , 0.   ],\n",
       "       [1.   , 0.   , 0.   ],\n",
       "       [1.   , 0.   , 0.   ],\n",
       "       [1.   , 0.   , 0.   ],\n",
       "       [1.   , 0.   , 0.   ],\n",
       "       [1.   , 0.   , 0.   ],\n",
       "       [1.   , 0.   , 0.   ],\n",
       "       [1.   , 0.   , 0.   ],\n",
       "       [1.   , 0.   , 0.   ],\n",
       "       [1.   , 0.   , 0.   ],\n",
       "       [1.   , 0.   , 0.   ],\n",
       "       [1.   , 0.   , 0.   ],\n",
       "       [1.   , 0.   , 0.   ],\n",
       "       [1.   , 0.   , 0.   ],\n",
       "       [1.   , 0.   , 0.   ],\n",
       "       [1.   , 0.   , 0.   ],\n",
       "       [1.   , 0.   , 0.   ],\n",
       "       [1.   , 0.   , 0.   ],\n",
       "       [1.   , 0.   , 0.   ],\n",
       "       [1.   , 0.   , 0.   ],\n",
       "       [1.   , 0.   , 0.   ],\n",
       "       [1.   , 0.   , 0.   ],\n",
       "       [1.   , 0.   , 0.   ],\n",
       "       [0.   , 1.   , 0.   ],\n",
       "       [0.   , 1.   , 0.   ],\n",
       "       [0.   , 1.   , 0.   ],\n",
       "       [0.   , 1.   , 0.   ],\n",
       "       [0.   , 1.   , 0.   ],\n",
       "       [0.   , 1.   , 0.   ],\n",
       "       [0.   , 1.   , 0.   ],\n",
       "       [0.   , 1.   , 0.   ],\n",
       "       [0.   , 1.   , 0.   ],\n",
       "       [0.   , 1.   , 0.   ],\n",
       "       [0.   , 1.   , 0.   ],\n",
       "       [0.   , 1.   , 0.   ],\n",
       "       [0.   , 1.   , 0.   ],\n",
       "       [0.   , 1.   , 0.   ],\n",
       "       [0.   , 1.   , 0.   ],\n",
       "       [0.   , 1.   , 0.   ],\n",
       "       [0.   , 1.   , 0.   ],\n",
       "       [0.   , 1.   , 0.   ],\n",
       "       [0.   , 1.   , 0.   ],\n",
       "       [0.   , 1.   , 0.   ],\n",
       "       [0.   , 0.55 , 0.45 ],\n",
       "       [0.   , 1.   , 0.   ],\n",
       "       [0.   , 0.725, 0.275],\n",
       "       [0.   , 1.   , 0.   ],\n",
       "       [0.   , 1.   , 0.   ],\n",
       "       [0.   , 1.   , 0.   ],\n",
       "       [0.   , 1.   , 0.   ],\n",
       "       [0.   , 0.25 , 0.75 ],\n",
       "       [0.   , 1.   , 0.   ],\n",
       "       [0.   , 1.   , 0.   ],\n",
       "       [0.   , 1.   , 0.   ],\n",
       "       [0.   , 1.   , 0.   ],\n",
       "       [0.   , 1.   , 0.   ],\n",
       "       [0.   , 0.075, 0.925],\n",
       "       [0.   , 1.   , 0.   ],\n",
       "       [0.   , 1.   , 0.   ],\n",
       "       [0.   , 1.   , 0.   ],\n",
       "       [0.   , 1.   , 0.   ],\n",
       "       [0.   , 1.   , 0.   ],\n",
       "       [0.   , 1.   , 0.   ],\n",
       "       [0.   , 1.   , 0.   ],\n",
       "       [0.   , 1.   , 0.   ],\n",
       "       [0.   , 1.   , 0.   ],\n",
       "       [0.   , 1.   , 0.   ],\n",
       "       [0.   , 1.   , 0.   ],\n",
       "       [0.   , 1.   , 0.   ],\n",
       "       [0.   , 1.   , 0.   ],\n",
       "       [0.   , 1.   , 0.   ],\n",
       "       [0.   , 1.   , 0.   ],\n",
       "       [0.   , 1.   , 0.   ],\n",
       "       [0.   , 0.   , 1.   ],\n",
       "       [0.   , 0.   , 1.   ],\n",
       "       [0.   , 0.   , 1.   ],\n",
       "       [0.   , 0.   , 1.   ],\n",
       "       [0.   , 0.   , 1.   ],\n",
       "       [0.   , 0.   , 1.   ],\n",
       "       [0.   , 0.75 , 0.25 ],\n",
       "       [0.   , 0.   , 1.   ],\n",
       "       [0.   , 0.   , 1.   ],\n",
       "       [0.   , 0.   , 1.   ],\n",
       "       [0.   , 0.   , 1.   ],\n",
       "       [0.   , 0.   , 1.   ],\n",
       "       [0.   , 0.   , 1.   ],\n",
       "       [0.   , 0.   , 1.   ],\n",
       "       [0.   , 0.   , 1.   ],\n",
       "       [0.   , 0.   , 1.   ],\n",
       "       [0.   , 0.   , 1.   ],\n",
       "       [0.   , 0.   , 1.   ],\n",
       "       [0.   , 0.   , 1.   ],\n",
       "       [0.   , 0.25 , 0.75 ],\n",
       "       [0.   , 0.   , 1.   ],\n",
       "       [0.   , 0.025, 0.975],\n",
       "       [0.   , 0.   , 1.   ],\n",
       "       [0.   , 0.125, 0.875],\n",
       "       [0.   , 0.   , 1.   ],\n",
       "       [0.   , 0.   , 1.   ],\n",
       "       [0.   , 0.4  , 0.6  ],\n",
       "       [0.   , 0.225, 0.775],\n",
       "       [0.   , 0.   , 1.   ],\n",
       "       [0.   , 0.   , 1.   ],\n",
       "       [0.   , 0.   , 1.   ],\n",
       "       [0.   , 0.   , 1.   ],\n",
       "       [0.   , 0.   , 1.   ],\n",
       "       [0.   , 0.3  , 0.7  ],\n",
       "       [0.   , 0.   , 1.   ],\n",
       "       [0.   , 0.   , 1.   ],\n",
       "       [0.   , 0.   , 1.   ],\n",
       "       [0.   , 0.   , 1.   ],\n",
       "       [0.   , 0.5  , 0.5  ],\n",
       "       [0.   , 0.   , 1.   ],\n",
       "       [0.   , 0.   , 1.   ],\n",
       "       [0.   , 0.   , 1.   ],\n",
       "       [0.   , 0.   , 1.   ],\n",
       "       [0.   , 0.   , 1.   ],\n",
       "       [0.   , 0.   , 1.   ],\n",
       "       [0.   , 0.   , 1.   ],\n",
       "       [0.   , 0.   , 1.   ],\n",
       "       [0.   , 0.   , 1.   ],\n",
       "       [0.   , 0.   , 1.   ],\n",
       "       [0.   , 0.   , 1.   ]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import cross_val_score\n",
    "iris = load_iris()\n",
    "clf = HeterogenousEnsembleClassifier(n_estimators=40, verbosity = 1)\n",
    "clf.fit(iris.data, iris.target)\n",
    "# clf.predict(iris.data)\n",
    "clf.predict_proba(iris.data)\n",
    "# print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform a simple test using the HeterogenousEnsembleClassifier on the Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36 number of parameter combinations generated for hyperparameter range {'kernel': ['rbf', 'linear'], 'C': array([0.1]), 'gamma': [0.1, 0.5], 'probability': [True]}.\n",
      "Fitting model 1 with parameters: {'kernel': 'linear', 'C': 0.1, 'probability': True}\n",
      "Fitting model 2 with parameters: {'kernel': 'rbf', 'probability': True}\n",
      "Fitting model 3 with parameters: {'C': 0.1, 'gamma': 0.5, 'probability': True}\n",
      "Fitting model 4 with parameters: {'C': 0.1}\n",
      "Fitting model 5 with parameters: {'C': 0.1, 'probability': True}\n",
      "Fitting model 6 with parameters: {'kernel': 'linear', 'C': 0.1, 'gamma': 0.1}\n",
      "Fitting model 7 with parameters: {}\n",
      "Fitting model 8 with parameters: {'kernel': 'rbf', 'C': 0.1, 'gamma': 0.5, 'probability': True}\n",
      "Fitting model 9 with parameters: {'kernel': 'linear', 'gamma': 0.5}\n",
      "Fitting model 10 with parameters: {'kernel': 'rbf', 'C': 0.1, 'gamma': 0.1}\n",
      "10 models fitted with base estimator SVC.\n",
      "Model no. 1 SVC(C=0.1, kernel='linear', probability=True) predict completed.\n",
      "Model no. 2 SVC(probability=True) predict completed.\n",
      "Model no. 3 SVC(C=0.1, gamma=0.5, probability=True) predict completed.\n",
      "Model no. 4 SVC(C=0.1) predict completed.\n",
      "Model no. 5 SVC(C=0.1, probability=True) predict completed.\n",
      "Model no. 6 SVC(C=0.1, gamma=0.1, kernel='linear') predict completed.\n",
      "Model no. 7 SVC() predict completed.\n",
      "Model no. 8 SVC(C=0.1, gamma=0.5, probability=True) predict completed.\n",
      "Model no. 9 SVC(gamma=0.5, kernel='linear') predict completed.\n",
      "Model no. 10 SVC(C=0.1, gamma=0.1) predict completed.\n",
      "Aggregating ensemble predictions.\n",
      "Multiple maximums for query 71, will select at random from [1, 2]\n",
      "Multiple maximums for query 78, will select at random from [1, 2]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        50\n",
      "           1       0.98      0.96      0.97        50\n",
      "           2       0.96      0.98      0.97        50\n",
      "\n",
      "    accuracy                           0.98       150\n",
      "   macro avg       0.98      0.98      0.98       150\n",
      "weighted avg       0.98      0.98      0.98       150\n",
      "\n",
      "Confusion Matrix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>50</td>\n",
       "      <td>49</td>\n",
       "      <td>51</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted   0   1   2  All\n",
       "True                      \n",
       "0          50   0   0   50\n",
       "1           0  48   2   50\n",
       "2           0   1  49   50\n",
       "All        50  49  51  150"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model no. 1 SVC(C=0.1, kernel='linear', probability=True) predict completed.\n",
      "Model no. 2 SVC(probability=True) predict completed.\n",
      "Model no. 3 SVC(C=0.1, gamma=0.5, probability=True) predict completed.\n",
      "Model no. 4 SVC(C=0.1) predict completed.\n",
      "Model no. 5 SVC(C=0.1, probability=True) predict completed.\n",
      "Model no. 6 SVC(C=0.1, gamma=0.1, kernel='linear') predict completed.\n",
      "Model no. 7 SVC() predict completed.\n",
      "Model no. 8 SVC(C=0.1, gamma=0.5, probability=True) predict completed.\n",
      "Model no. 9 SVC(gamma=0.5, kernel='linear') predict completed.\n",
      "Model no. 10 SVC(C=0.1, gamma=0.1) predict completed.\n",
      "Calculating probabilities from ensemble predictions\n",
      "Probabilities calculated for 150 queries and 3 classes\n",
      "Model no. 1 SVC(C=0.1, kernel='linear', probability=True) predict completed.\n",
      "Model no. 2 SVC(probability=True) predict completed.\n",
      "Model no. 3 SVC(C=0.1, gamma=0.5, probability=True) predict completed.\n",
      "Model no. 4 SVC(C=0.1) predict completed.\n",
      "Model no. 5 SVC(C=0.1, probability=True) predict completed.\n",
      "Model no. 6 SVC(C=0.1, gamma=0.1, kernel='linear') predict completed.\n",
      "Model no. 7 SVC() predict completed.\n",
      "Model no. 8 SVC(C=0.1, gamma=0.5, probability=True) predict completed.\n",
      "Model no. 9 SVC(gamma=0.5, kernel='linear') predict completed.\n",
      "Model no. 10 SVC(C=0.1, gamma=0.1) predict completed.\n",
      "Aggregating ensemble predictions.\n",
      "Multiple maximums for query 71, will select at random from [1, 2]\n",
      "Multiple maximums for query 78, will select at random from [1, 2]\n"
     ]
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "base_estimator = svm.SVC()\n",
    "hyperparam_range = {\"kernel\":[\"rbf\", \"linear\"], \"C\":np.arange(0.1, 1.0, 10), \"gamma\":[0.1, 0.5], \"probability\":[True]}\n",
    "n_estimators = 10\n",
    "clf = HeterogenousEnsembleClassifier(base_estimator, n_estimators, hyperparam_range, verbosity = 2)\n",
    "clf.fit(iris.data, iris.target)\n",
    "y_pred = clf.predict(iris.data)\n",
    "print(metrics.classification_report(iris.target, y_pred))\n",
    "print(\"Confusion Matrix\")\n",
    "display(pd.crosstab(np.array(iris.target), y_pred, rownames=['True'], colnames=['Predicted'], margins=True))\n",
    "y_pred = clf.predict_proba(iris.data)\n",
    "y_pred2 = clf.predict(iris.data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform a cross validation experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36 number of parameter combinations generated for hyperparameter range {'kernel': ['rbf', 'linear'], 'C': array([0.1]), 'gamma': [0.1, 0.5], 'probability': [True]}.\n",
      "Fitting model 1 with parameters: {'kernel': 'rbf', 'C': 0.1, 'gamma': 0.1, 'probability': True}\n",
      "Fitting model 2 with parameters: {'kernel': 'rbf', 'probability': True}\n",
      "Fitting model 3 with parameters: {'kernel': 'linear', 'gamma': 0.5}\n",
      "Fitting model 4 with parameters: {'kernel': 'rbf', 'gamma': 0.5}\n",
      "Fitting model 5 with parameters: {'kernel': 'linear', 'gamma': 0.5, 'probability': True}\n",
      "Fitting model 6 with parameters: {'kernel': 'linear', 'C': 0.1}\n",
      "Fitting model 7 with parameters: {'kernel': 'linear'}\n",
      "Fitting model 8 with parameters: {'C': 0.1}\n",
      "Fitting model 9 with parameters: {'kernel': 'rbf', 'C': 0.1, 'gamma': 0.5, 'probability': True}\n",
      "Fitting model 10 with parameters: {'kernel': 'linear', 'gamma': 0.1, 'probability': True}\n",
      "10 models fitted with base estimator SVC.\n",
      "Model no. 1 SVC(C=0.1, gamma=0.1, probability=True) predict completed.\n",
      "Model no. 2 SVC(probability=True) predict completed.\n",
      "Model no. 3 SVC(gamma=0.5, kernel='linear') predict completed.\n",
      "Model no. 4 SVC(gamma=0.5) predict completed.\n",
      "Model no. 5 SVC(gamma=0.5, kernel='linear', probability=True) predict completed.\n",
      "Model no. 6 SVC(C=0.1, kernel='linear') predict completed.\n",
      "Model no. 7 SVC(kernel='linear') predict completed.\n",
      "Model no. 8 SVC(C=0.1) predict completed.\n",
      "Model no. 9 SVC(C=0.1, gamma=0.5, probability=True) predict completed.\n",
      "Model no. 10 SVC(gamma=0.1, kernel='linear', probability=True) predict completed.\n",
      "Aggregating ensemble predictions.\n",
      "36 number of parameter combinations generated for hyperparameter range {'kernel': ['rbf', 'linear'], 'C': array([0.1]), 'gamma': [0.1, 0.5], 'probability': [True]}.\n",
      "Fitting model 1 with parameters: {'kernel': 'rbf', 'C': 0.1, 'gamma': 0.5, 'probability': True}\n",
      "Fitting model 2 with parameters: {'C': 0.1, 'gamma': 0.1, 'probability': True}\n",
      "Fitting model 3 with parameters: {'kernel': 'linear', 'C': 0.1, 'gamma': 0.1, 'probability': True}\n",
      "Fitting model 4 with parameters: {'kernel': 'linear', 'gamma': 0.5}\n",
      "Fitting model 5 with parameters: {'probability': True}\n",
      "Fitting model 6 with parameters: {'kernel': 'rbf', 'C': 0.1, 'gamma': 0.5, 'probability': True}\n",
      "Fitting model 7 with parameters: {'C': 0.1, 'gamma': 0.5, 'probability': True}\n",
      "Fitting model 8 with parameters: {'gamma': 0.5}\n",
      "Fitting model 9 with parameters: {'kernel': 'linear', 'gamma': 0.5}\n",
      "Fitting model 10 with parameters: {'gamma': 0.5}\n",
      "10 models fitted with base estimator SVC.\n",
      "Model no. 1 SVC(C=0.1, gamma=0.5, probability=True) predict completed.\n",
      "Model no. 2 SVC(C=0.1, gamma=0.1, probability=True) predict completed.\n",
      "Model no. 3 SVC(C=0.1, gamma=0.1, kernel='linear', probability=True) predict completed.\n",
      "Model no. 4 SVC(gamma=0.5, kernel='linear') predict completed.\n",
      "Model no. 5 SVC(probability=True) predict completed.\n",
      "Model no. 6 SVC(C=0.1, gamma=0.5, probability=True) predict completed.\n",
      "Model no. 7 SVC(C=0.1, gamma=0.5, probability=True) predict completed.\n",
      "Model no. 8 SVC(gamma=0.5) predict completed.\n",
      "Model no. 9 SVC(gamma=0.5, kernel='linear') predict completed.\n",
      "Model no. 10 SVC(gamma=0.5) predict completed.\n",
      "Aggregating ensemble predictions.\n",
      "36 number of parameter combinations generated for hyperparameter range {'kernel': ['rbf', 'linear'], 'C': array([0.1]), 'gamma': [0.1, 0.5], 'probability': [True]}.\n",
      "Fitting model 1 with parameters: {}\n",
      "Fitting model 2 with parameters: {'kernel': 'rbf', 'C': 0.1, 'gamma': 0.5}\n",
      "Fitting model 3 with parameters: {'kernel': 'rbf', 'C': 0.1, 'gamma': 0.5}\n",
      "Fitting model 4 with parameters: {'kernel': 'linear', 'C': 0.1, 'gamma': 0.5}\n",
      "Fitting model 5 with parameters: {'gamma': 0.1}\n",
      "Fitting model 6 with parameters: {'kernel': 'linear', 'probability': True}\n",
      "Fitting model 7 with parameters: {'kernel': 'rbf', 'gamma': 0.5}\n",
      "Fitting model 8 with parameters: {'kernel': 'rbf', 'probability': True}\n",
      "Fitting model 9 with parameters: {'gamma': 0.5, 'probability': True}\n",
      "Fitting model 10 with parameters: {'kernel': 'rbf', 'gamma': 0.5, 'probability': True}\n",
      "10 models fitted with base estimator SVC.\n",
      "Model no. 1 SVC() predict completed.\n",
      "Model no. 2 SVC(C=0.1, gamma=0.5) predict completed.\n",
      "Model no. 3 SVC(C=0.1, gamma=0.5) predict completed.\n",
      "Model no. 4 SVC(C=0.1, gamma=0.5, kernel='linear') predict completed.\n",
      "Model no. 5 SVC(gamma=0.1) predict completed.\n",
      "Model no. 6 SVC(kernel='linear', probability=True) predict completed.\n",
      "Model no. 7 SVC(gamma=0.5) predict completed.\n",
      "Model no. 8 SVC(probability=True) predict completed.\n",
      "Model no. 9 SVC(gamma=0.5, probability=True) predict completed.\n",
      "Model no. 10 SVC(gamma=0.5, probability=True) predict completed.\n",
      "Aggregating ensemble predictions.\n",
      "36 number of parameter combinations generated for hyperparameter range {'kernel': ['rbf', 'linear'], 'C': array([0.1]), 'gamma': [0.1, 0.5], 'probability': [True]}.\n",
      "Fitting model 1 with parameters: {'kernel': 'linear', 'gamma': 0.5, 'probability': True}\n",
      "Fitting model 2 with parameters: {'kernel': 'rbf', 'C': 0.1, 'gamma': 0.1, 'probability': True}\n",
      "Fitting model 3 with parameters: {'kernel': 'linear', 'gamma': 0.5, 'probability': True}\n",
      "Fitting model 4 with parameters: {'kernel': 'linear', 'C': 0.1}\n",
      "Fitting model 5 with parameters: {'kernel': 'rbf', 'gamma': 0.1}\n",
      "Fitting model 6 with parameters: {'kernel': 'linear', 'gamma': 0.5}\n",
      "Fitting model 7 with parameters: {'kernel': 'linear', 'C': 0.1, 'probability': True}\n",
      "Fitting model 8 with parameters: {'kernel': 'linear', 'gamma': 0.1, 'probability': True}\n",
      "Fitting model 9 with parameters: {'kernel': 'linear', 'gamma': 0.5}\n",
      "Fitting model 10 with parameters: {'C': 0.1, 'gamma': 0.1, 'probability': True}\n",
      "10 models fitted with base estimator SVC.\n",
      "Model no. 1 SVC(gamma=0.5, kernel='linear', probability=True) predict completed.\n",
      "Model no. 2 SVC(C=0.1, gamma=0.1, probability=True) predict completed.\n",
      "Model no. 3 SVC(gamma=0.5, kernel='linear', probability=True) predict completed.\n",
      "Model no. 4 SVC(C=0.1, kernel='linear') predict completed.\n",
      "Model no. 5 SVC(gamma=0.1) predict completed.\n",
      "Model no. 6 SVC(gamma=0.5, kernel='linear') predict completed.\n",
      "Model no. 7 SVC(C=0.1, kernel='linear', probability=True) predict completed.\n",
      "Model no. 8 SVC(gamma=0.1, kernel='linear', probability=True) predict completed.\n",
      "Model no. 9 SVC(gamma=0.5, kernel='linear') predict completed.\n",
      "Model no. 10 SVC(C=0.1, gamma=0.1, probability=True) predict completed.\n",
      "Aggregating ensemble predictions.\n",
      "36 number of parameter combinations generated for hyperparameter range {'kernel': ['rbf', 'linear'], 'C': array([0.1]), 'gamma': [0.1, 0.5], 'probability': [True]}.\n",
      "Fitting model 1 with parameters: {'kernel': 'rbf', 'C': 0.1, 'probability': True}\n",
      "Fitting model 2 with parameters: {}\n",
      "Fitting model 3 with parameters: {'kernel': 'linear', 'gamma': 0.1}\n",
      "Fitting model 4 with parameters: {}\n",
      "Fitting model 5 with parameters: {'kernel': 'rbf', 'gamma': 0.5}\n",
      "Fitting model 6 with parameters: {'kernel': 'rbf', 'gamma': 0.5}\n",
      "Fitting model 7 with parameters: {'kernel': 'rbf', 'gamma': 0.5}\n",
      "Fitting model 8 with parameters: {'kernel': 'rbf'}\n",
      "Fitting model 9 with parameters: {'kernel': 'rbf', 'C': 0.1, 'gamma': 0.5}\n",
      "Fitting model 10 with parameters: {'kernel': 'rbf', 'gamma': 0.5}\n",
      "10 models fitted with base estimator SVC.\n",
      "Model no. 1 SVC(C=0.1, probability=True) predict completed.\n",
      "Model no. 2 SVC() predict completed.\n",
      "Model no. 3 SVC(gamma=0.1, kernel='linear') predict completed.\n",
      "Model no. 4 SVC() predict completed.\n",
      "Model no. 5 SVC(gamma=0.5) predict completed.\n",
      "Model no. 6 SVC(gamma=0.5) predict completed.\n",
      "Model no. 7 SVC(gamma=0.5) predict completed.\n",
      "Model no. 8 SVC() predict completed.\n",
      "Model no. 9 SVC(C=0.1, gamma=0.5) predict completed.\n",
      "Model no. 10 SVC(gamma=0.5) predict completed.\n",
      "Aggregating ensemble predictions.\n",
      "Multiple maximums for query 6, will select at random from [1, 2]\n",
      "36 number of parameter combinations generated for hyperparameter range {'kernel': ['rbf', 'linear'], 'C': array([0.1]), 'gamma': [0.1, 0.5], 'probability': [True]}.\n",
      "Fitting model 1 with parameters: {'C': 0.1, 'gamma': 0.1, 'probability': True}\n",
      "Fitting model 2 with parameters: {'C': 0.1, 'gamma': 0.5, 'probability': True}\n",
      "Fitting model 3 with parameters: {'kernel': 'rbf', 'C': 0.1, 'gamma': 0.5}\n",
      "Fitting model 4 with parameters: {'kernel': 'linear', 'C': 0.1, 'gamma': 0.5, 'probability': True}\n",
      "Fitting model 5 with parameters: {'C': 0.1, 'probability': True}\n",
      "Fitting model 6 with parameters: {'kernel': 'rbf', 'C': 0.1, 'gamma': 0.5, 'probability': True}\n",
      "Fitting model 7 with parameters: {}\n",
      "Fitting model 8 with parameters: {'kernel': 'linear', 'gamma': 0.5}\n",
      "Fitting model 9 with parameters: {'kernel': 'rbf', 'C': 0.1, 'gamma': 0.5}\n",
      "Fitting model 10 with parameters: {'C': 0.1, 'gamma': 0.1, 'probability': True}\n",
      "10 models fitted with base estimator SVC.\n",
      "Model no. 1 SVC(C=0.1, gamma=0.1, probability=True) predict completed.\n",
      "Model no. 2 SVC(C=0.1, gamma=0.5, probability=True) predict completed.\n",
      "Model no. 3 SVC(C=0.1, gamma=0.5) predict completed.\n",
      "Model no. 4 SVC(C=0.1, gamma=0.5, kernel='linear', probability=True) predict completed.\n",
      "Model no. 5 SVC(C=0.1, probability=True) predict completed.\n",
      "Model no. 6 SVC(C=0.1, gamma=0.5, probability=True) predict completed.\n",
      "Model no. 7 SVC() predict completed.\n",
      "Model no. 8 SVC(gamma=0.5, kernel='linear') predict completed.\n",
      "Model no. 9 SVC(C=0.1, gamma=0.5) predict completed.\n",
      "Model no. 10 SVC(C=0.1, gamma=0.1, probability=True) predict completed.\n",
      "Aggregating ensemble predictions.\n",
      "36 number of parameter combinations generated for hyperparameter range {'kernel': ['rbf', 'linear'], 'C': array([0.1]), 'gamma': [0.1, 0.5], 'probability': [True]}.\n",
      "Fitting model 1 with parameters: {'C': 0.1, 'gamma': 0.5}\n",
      "Fitting model 2 with parameters: {'kernel': 'rbf', 'probability': True}\n",
      "Fitting model 3 with parameters: {'kernel': 'rbf', 'gamma': 0.1}\n",
      "Fitting model 4 with parameters: {'kernel': 'linear', 'gamma': 0.1, 'probability': True}\n",
      "Fitting model 5 with parameters: {'C': 0.1, 'gamma': 0.5, 'probability': True}\n",
      "Fitting model 6 with parameters: {'C': 0.1, 'gamma': 0.5, 'probability': True}\n",
      "Fitting model 7 with parameters: {'kernel': 'rbf', 'C': 0.1, 'probability': True}\n",
      "Fitting model 8 with parameters: {'kernel': 'linear', 'gamma': 0.5}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model 9 with parameters: {'C': 0.1, 'gamma': 0.1}\n",
      "Fitting model 10 with parameters: {'C': 0.1}\n",
      "10 models fitted with base estimator SVC.\n",
      "Model no. 1 SVC(C=0.1, gamma=0.5) predict completed.\n",
      "Model no. 2 SVC(probability=True) predict completed.\n",
      "Model no. 3 SVC(gamma=0.1) predict completed.\n",
      "Model no. 4 SVC(gamma=0.1, kernel='linear', probability=True) predict completed.\n",
      "Model no. 5 SVC(C=0.1, gamma=0.5, probability=True) predict completed.\n",
      "Model no. 6 SVC(C=0.1, gamma=0.5, probability=True) predict completed.\n",
      "Model no. 7 SVC(C=0.1, probability=True) predict completed.\n",
      "Model no. 8 SVC(gamma=0.5, kernel='linear') predict completed.\n",
      "Model no. 9 SVC(C=0.1, gamma=0.1) predict completed.\n",
      "Model no. 10 SVC(C=0.1) predict completed.\n",
      "Aggregating ensemble predictions.\n",
      "36 number of parameter combinations generated for hyperparameter range {'kernel': ['rbf', 'linear'], 'C': array([0.1]), 'gamma': [0.1, 0.5], 'probability': [True]}.\n",
      "Fitting model 1 with parameters: {'C': 0.1, 'gamma': 0.5}\n",
      "Fitting model 2 with parameters: {'kernel': 'linear', 'gamma': 0.1}\n",
      "Fitting model 3 with parameters: {'kernel': 'rbf', 'gamma': 0.1, 'probability': True}\n",
      "Fitting model 4 with parameters: {'kernel': 'rbf', 'gamma': 0.1}\n",
      "Fitting model 5 with parameters: {'kernel': 'linear', 'C': 0.1, 'probability': True}\n",
      "Fitting model 6 with parameters: {'kernel': 'linear', 'C': 0.1}\n",
      "Fitting model 7 with parameters: {'kernel': 'rbf', 'C': 0.1, 'gamma': 0.5, 'probability': True}\n",
      "Fitting model 8 with parameters: {'kernel': 'rbf', 'gamma': 0.1}\n",
      "Fitting model 9 with parameters: {'kernel': 'rbf', 'probability': True}\n",
      "Fitting model 10 with parameters: {'C': 0.1, 'gamma': 0.5, 'probability': True}\n",
      "10 models fitted with base estimator SVC.\n",
      "Model no. 1 SVC(C=0.1, gamma=0.5) predict completed.\n",
      "Model no. 2 SVC(gamma=0.1, kernel='linear') predict completed.\n",
      "Model no. 3 SVC(gamma=0.1, probability=True) predict completed.\n",
      "Model no. 4 SVC(gamma=0.1) predict completed.\n",
      "Model no. 5 SVC(C=0.1, kernel='linear', probability=True) predict completed.\n",
      "Model no. 6 SVC(C=0.1, kernel='linear') predict completed.\n",
      "Model no. 7 SVC(C=0.1, gamma=0.5, probability=True) predict completed.\n",
      "Model no. 8 SVC(gamma=0.1) predict completed.\n",
      "Model no. 9 SVC(probability=True) predict completed.\n",
      "Model no. 10 SVC(C=0.1, gamma=0.5, probability=True) predict completed.\n",
      "Aggregating ensemble predictions.\n",
      "36 number of parameter combinations generated for hyperparameter range {'kernel': ['rbf', 'linear'], 'C': array([0.1]), 'gamma': [0.1, 0.5], 'probability': [True]}.\n",
      "Fitting model 1 with parameters: {'kernel': 'rbf', 'C': 0.1, 'gamma': 0.1, 'probability': True}\n",
      "Fitting model 2 with parameters: {'gamma': 0.5}\n",
      "Fitting model 3 with parameters: {'kernel': 'linear', 'C': 0.1, 'probability': True}\n",
      "Fitting model 4 with parameters: {'kernel': 'rbf', 'gamma': 0.5}\n",
      "Fitting model 5 with parameters: {'kernel': 'linear', 'gamma': 0.1}\n",
      "Fitting model 6 with parameters: {'kernel': 'linear', 'C': 0.1, 'probability': True}\n",
      "Fitting model 7 with parameters: {'kernel': 'rbf', 'C': 0.1, 'probability': True}\n",
      "Fitting model 8 with parameters: {'kernel': 'linear', 'probability': True}\n",
      "Fitting model 9 with parameters: {'kernel': 'linear', 'C': 0.1, 'gamma': 0.5, 'probability': True}\n",
      "Fitting model 10 with parameters: {'kernel': 'rbf', 'C': 0.1, 'gamma': 0.1, 'probability': True}\n",
      "10 models fitted with base estimator SVC.\n",
      "Model no. 1 SVC(C=0.1, gamma=0.1, probability=True) predict completed.\n",
      "Model no. 2 SVC(gamma=0.5) predict completed.\n",
      "Model no. 3 SVC(C=0.1, kernel='linear', probability=True) predict completed.\n",
      "Model no. 4 SVC(gamma=0.5) predict completed.\n",
      "Model no. 5 SVC(gamma=0.1, kernel='linear') predict completed.\n",
      "Model no. 6 SVC(C=0.1, kernel='linear', probability=True) predict completed.\n",
      "Model no. 7 SVC(C=0.1, probability=True) predict completed.\n",
      "Model no. 8 SVC(kernel='linear', probability=True) predict completed.\n",
      "Model no. 9 SVC(C=0.1, gamma=0.5, kernel='linear', probability=True) predict completed.\n",
      "Model no. 10 SVC(C=0.1, gamma=0.1, probability=True) predict completed.\n",
      "Aggregating ensemble predictions.\n",
      "36 number of parameter combinations generated for hyperparameter range {'kernel': ['rbf', 'linear'], 'C': array([0.1]), 'gamma': [0.1, 0.5], 'probability': [True]}.\n",
      "Fitting model 1 with parameters: {'kernel': 'linear', 'C': 0.1, 'gamma': 0.1}\n",
      "Fitting model 2 with parameters: {'kernel': 'rbf'}\n",
      "Fitting model 3 with parameters: {'kernel': 'rbf', 'C': 0.1}\n",
      "Fitting model 4 with parameters: {'kernel': 'linear', 'probability': True}\n",
      "Fitting model 5 with parameters: {'probability': True}\n",
      "Fitting model 6 with parameters: {'kernel': 'linear', 'gamma': 0.1}\n",
      "Fitting model 7 with parameters: {'kernel': 'rbf', 'C': 0.1, 'gamma': 0.1}\n",
      "Fitting model 8 with parameters: {'probability': True}\n",
      "Fitting model 9 with parameters: {'kernel': 'linear'}\n",
      "Fitting model 10 with parameters: {'kernel': 'linear'}\n",
      "10 models fitted with base estimator SVC.\n",
      "Model no. 1 SVC(C=0.1, gamma=0.1, kernel='linear') predict completed.\n",
      "Model no. 2 SVC() predict completed.\n",
      "Model no. 3 SVC(C=0.1) predict completed.\n",
      "Model no. 4 SVC(kernel='linear', probability=True) predict completed.\n",
      "Model no. 5 SVC(probability=True) predict completed.\n",
      "Model no. 6 SVC(gamma=0.1, kernel='linear') predict completed.\n",
      "Model no. 7 SVC(C=0.1, gamma=0.1) predict completed.\n",
      "Model no. 8 SVC(probability=True) predict completed.\n",
      "Model no. 9 SVC(kernel='linear') predict completed.\n",
      "Model no. 10 SVC(kernel='linear') predict completed.\n",
      "Aggregating ensemble predictions.\n",
      "[1.         0.93333333 1.         1.         1.         0.93333333\n",
      " 0.93333333 1.         1.         1.        ]\n",
      "0.9800000000000001  +/-  0.030550504633038926\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(clf, iris.data, iris.target, cv=10)\n",
    "print(scores)\n",
    "print(np.mean(scores), \" +/- \", np.std(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with imbalanced, non numeric targest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Year</th>\n",
       "      <th>NNodes</th>\n",
       "      <th>Class</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>GE5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30</td>\n",
       "      <td>62</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>GE5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30</td>\n",
       "      <td>65</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>GE5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31</td>\n",
       "      <td>59</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>GE5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31</td>\n",
       "      <td>65</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>GE5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age  Year  NNodes  Class Survived\n",
       "0   30    64       1      1      GE5\n",
       "1   30    62       3      1      GE5\n",
       "2   30    65       0      1      GE5\n",
       "3   31    59       2      1      GE5\n",
       "4   31    65       4      1      GE5"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "surv = pd.read_csv('survival.csv')\n",
    "surv['Survived'] = 'GE5'\n",
    "surv.loc[surv['Class']==2,'Survived']='L5'\n",
    "surv.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((306, 3), (306,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = surv.pop('Survived').values\n",
    "surv.pop('Class')\n",
    "X = surv.values\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 models fitted with base estimator SVC.\n",
      "Predict completed for 10 models \n",
      "Results aggregated for 306 queries\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>GE5</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GE5</th>\n",
       "      <td>225</td>\n",
       "      <td>225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L5</th>\n",
       "      <td>81</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>306</td>\n",
       "      <td>306</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted  GE5  All\n",
       "True               \n",
       "GE5        225  225\n",
       "L5          81   81\n",
       "All        306  306"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         GE5       0.74      1.00      0.85       225\n",
      "          L5       0.00      0.00      0.00        81\n",
      "\n",
      "    accuracy                           0.74       306\n",
      "   macro avg       0.37      0.50      0.42       306\n",
      "weighted avg       0.54      0.74      0.62       306\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base_estimator = svm.SVC()\n",
    "hyperparam_range = {\"kernel\":[\"rbf\", \"linear\"], \"C\":np.arange(0.1, 1.0, 10), \"gamma\":[0.1, 0.5], \"probability\":[True]}\n",
    "n_estimators = 10\n",
    "clf = HeterogenousEnsembleClassifier(base_estimator, n_estimators, hyperparam_range, verbosity = 1)\n",
    "clf.fit(X, y)\n",
    "y_pred = clf.predict(X)\n",
    "display(pd.crosstab(y, y_pred, rownames=['True'], colnames=['Predicted'], margins=True))\n",
    "print(metrics.classification_report(y, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 4)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: The StackedHeterogenousEnsembleClassifier Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define StackedHeterogenousEnsembleClassifier Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new classifier which is based on the sckit-learn BaseEstimator and ClassifierMixin classes\n",
    "class StackedHeterogenousEnsembleClassifier(BaseEstimator, ClassifierMixin):\n",
    "    \n",
    "    \"\"\"An ensemble classifier that uses heterogeneous models at the base layer. Base models are different due to different hyper-parameters used. Aggrefgattion is perfomred using a stack layer model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    base_estimator: scikit-learn estimator \n",
    "        The model type to be used at the base layer of the ensemble model.\n",
    "\n",
    "    hp_range_map: dictionary\n",
    "        A dictinary of hyperparamters and the ranges of values that will be used from them\n",
    "        \n",
    "    n_estimators: int\n",
    "        How many models to use in the ensemble\n",
    "        \n",
    "    bootstrap: boolean\n",
    "        Whether or not to use bootstrap sampling wehn training base estimators\n",
    "    \n",
    "    stack_layer_estimator: scikit-learn estimator \n",
    "        Estimator type of the stack  layer model\n",
    "        \n",
    "    base_stack_data_ratio: float\n",
    "        The ratio with which to split the data for straing the base and stack layers.\n",
    "        \n",
    "    Attributes\n",
    "    ----------\n",
    "    classes_ : array of shape = [n_classes] \n",
    "        The classes labels.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    The default values for most base learners are used, unless hyperparameter ranges are specified\n",
    "\n",
    "    See also\n",
    "    --------\n",
    "    \n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> from sklearn.datasets import load_iris\n",
    "    >>> from sklearn.model_selection import cross_val_score\n",
    "    >>> clf = StackedHeterogenousEnsembleClassifier(tree.DecisionTreeClassifier(), {'max_depth':[5, 10, 15], })\n",
    "    >>> iris = load_iris()\n",
    "    >>> cross_val_score(clf, iris.data, iris.target, cv=10)\n",
    "\n",
    "    \"\"\"\n",
    "    # Constructor for the classifier object\n",
    "    def __init__(self, base_estimator = svm.SVC(), n_estimators = 10, hp_range_map = None, bootstrap = True, stack_layer_estimator = svm.SVC(), base_stack_data_ratio = 0.7, random_state=None, verbosity = 0):\n",
    "\n",
    "        \"\"\"Setup a StackedHeterogenousEnsembleClassifier classifier .\n",
    "        Parameters\n",
    "        ----------\n",
    "        base_estimator: The model type to be used at the base layer of the ensemble model.\n",
    "        hp_range_map: A dictinary of hyperparamters and the ranges of values that will be used from them\n",
    "        n_estimators: How many models to use in the ensemble\n",
    "        bootstrap: Wheter or not to use bootstrap sampling wehn training base estimators\n",
    "        stack_layer_estimator: Estimator type of the stack  layer model\n",
    "        base_stack_data_ratio: The ratio with which to split the data for straing the base and stack layers.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        The estimator\n",
    "        \"\"\"     \n",
    "\n",
    "        # Initialise ranomd state if set\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        # Initialise class variabels\n",
    "        self.base_estimator = base_estimator\n",
    "        self.hp_range_map = hp_range_map\n",
    "        self.n_estimators = n_estimators\n",
    "        self.bootstrap = bootstrap\n",
    "        self.stack_layer_estimator = stack_layer_estimator\n",
    "        self.base_stack_data_ratio = base_stack_data_ratio\n",
    "        self.verbosity = verbosity\n",
    "        \n",
    "        \n",
    "    \n",
    "    def validate_parameters(self):\n",
    "        if self.n_estimators < 1:\n",
    "             raise ValueError(\"n_estimators must be >= 1\")\n",
    "        if is_classifier(self.base_estimator) is False:\n",
    "            raise ValueError(\"base_estimator must be a classifier\")\n",
    "        if is_classifier(self.stack_layer_estimator) is False:\n",
    "            raise ValueError(\"stack_layer_estimator must be a classifier\")\n",
    "        if self.verbosity not in range(0,3):\n",
    "            raise ValueError(\"verbosity has range 0-2\")\n",
    "        if self.base_stack_data_ratio <= 0 or self.base_stack_data_ratio >= 1:\n",
    "            raise ValueError(\"base_stack_data_ratio must be greater than 0 and smaler than 1.\")\n",
    "        if self.hp_range_map is None:\n",
    "            self.hp_range_map = {}\n",
    "\n",
    "    # The fit function to train a classifier\n",
    "    def fit(self, X, y):\n",
    "\n",
    "        self.validate_parameters()\n",
    "    \n",
    "        def checkBootstrap(X, y):\n",
    "            if self.bootstrap is True:\n",
    "                return resample(X,y, replace=True)\n",
    "            return X,y \n",
    "        \n",
    "        def checkValue(value):\n",
    "            if type(value) is np.ndarray:\n",
    "                value = value.tolist()\n",
    "            elif type(value) is not list:\n",
    "                value = [value]\n",
    "            return value + [None]\n",
    "    \n",
    "        def generate_param_combinations():\n",
    "            params = []\n",
    "            for key, value in self.hp_range_map.items():\n",
    "                params.append(checkValue(value))\n",
    "                self.keys.append(key)\n",
    "            product = itertools.product(*params)\n",
    "            self.params = [ [ p for p in params ] for params in product ] \n",
    "            if self.verbosity == 2:\n",
    "                print(\"{} number of parameter combinations generated for hyperparameter range {}.\".format(len(self.params), self.hp_range_map))\n",
    "        \n",
    "        def extract_params(n):\n",
    "            params = {}\n",
    "            for i,key in enumerate(self.keys):\n",
    "                if self.params[n][i] is not None:\n",
    "                    params[key] = self.params[n][i]\n",
    "            return params\n",
    "    \n",
    "        def fit_ensemble(X_train, X_valid, y_train):\n",
    "            for i in range(0, self.n_estimators):\n",
    "                X_train, y_train = checkBootstrap(X_train, y_train)\n",
    "                params = extract_params(random.randint(0, (len(self.params)-1)))\n",
    "                clf = copy(self.base_estimator).set_params(**params)\n",
    "                if self.verbosity == 2:\n",
    "                    print(\"Fitting model {} with parameters: {}\".format(i+1, params))\n",
    "                clf.fit(X_train, y_train)\n",
    "                self.models.append(clf)\n",
    "                self.model_output.append(clf.predict(X_valid))\n",
    "            self.model_output = np.asarray(self.model_output)\n",
    "            if self.verbosity > 0:\n",
    "                print(\"{} models fitted with base estimator {}.\".format(i+1, self.base_estimator.__class__.__name__))\n",
    "        \n",
    "        def prepare_stack_training_set():\n",
    "            \n",
    "            self.model_output = [to_categorical(m, num_classes=len(self.classes_)+1) for m in self.model_output.transpose()]\n",
    "    \n",
    "            self.model_output = np.asarray(self.model_output)\n",
    "    \n",
    "            self.model_output = self.model_output.reshape(X_valid.shape[0], self.n_estimators*(len(self.classes_)+1))\n",
    "        \n",
    "            if self.verbosity > 0:\n",
    "                print(\"Training set of size {} prepared for stack layer\".format(self.model_output.size))\n",
    "            \n",
    "        # Check that X and y have correct shape\n",
    "        X, y = check_X_y(X, y)\n",
    "        \n",
    "        X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=self.base_stack_data_ratio,\n",
    "                                                              random_state=self.random_state)\n",
    "        if self.verbosity > 0:\n",
    "            print(\"X_train of size {} split at {} ratio\". format(X.size, self.base_stack_data_ratio))\n",
    "        \n",
    "        if self.verbosity == 2:\n",
    "            print(\"X_train for ensemble now size {}, X_train for stack now size {}\".format(X_train.size, X_valid.size))\n",
    "        \n",
    "        random.seed(self.random_state)\n",
    "        \n",
    "        self.params, self.keys, self.models, self.model_output = [], [], [], []\n",
    "                \n",
    "       #  Count the number of occurrences of each class in the target vector (uses mupy unique function that returns a list of unique values and their counts)\n",
    "        self.classes_ = np.unique(y)\n",
    "\n",
    "        generate_param_combinations()\n",
    "        \n",
    "        fit_ensemble(X_train, X_valid, y_train)\n",
    "    \n",
    "        prepare_stack_training_set()        \n",
    "        \n",
    "        self.stack_layer_estimator.fit(self.model_output, y_valid)\n",
    "        \n",
    "        if self.verbosity > 0:\n",
    "            print(\"Stack layer with base estimator {} has been fit\".format(self.stack_layer_estimator.__class__.__name__))\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def ensemble_predict(self, X):\n",
    "        for i, model in enumerate(self.models):\n",
    "            self.model_output.append(model.predict(X))\n",
    "            if self.verbosity == 2:\n",
    "                print('Model no. {} {} predict completed.'.format(i+1, model))\n",
    "        if self.verbosity > 0:\n",
    "            print(\"Predict completed for {} models \".format(i+1))\n",
    "        self.model_output = np.asarray(self.model_output)  \n",
    "\n",
    "        \n",
    "    def prepare_data_for_stack(self, X):\n",
    "        \n",
    "        if self.verbosity > 0:\n",
    "            print(\"Preparing data for stack layer.\")\n",
    "            \n",
    "        for i, prediction in enumerate(self.model_output.transpose()):\n",
    "            self.ensemble_predictions_.append(to_categorical(prediction, num_classes=len(self.classes_)+1))\n",
    "        \n",
    "        self.ensemble_predictions_ = np.array(self.ensemble_predictions_)\n",
    "        \n",
    "        self.ensemble_predictions_ = self.ensemble_predictions_.reshape(X.shape[0], (len(self.classes_)+1) * self.n_estimators)\n",
    "\n",
    "        if self.verbosity > 0:\n",
    "                print(\"Data set of size {} prepared for stack layer\".format(self.model_output.size))\n",
    "        #     \n",
    "# The predict function to make a set of predictions for a set of query instances\n",
    "    def predict(self, X):\n",
    "        \n",
    "        check_is_fitted(self, ['models'])\n",
    "        \n",
    "        self.ensemble_predictions_, self.model_output = [], []\n",
    "        \n",
    "        # Check that the input features match the type and shape of the training features\n",
    "        X = check_array(X)\n",
    "    \n",
    "        self.ensemble_predict(X)\n",
    "        \n",
    "        self.prepare_data_for_stack(X)\n",
    "        \n",
    "        if self.verbosity == 2:\n",
    "            print(\"Stack layer with base_estimator {} making prediction\".format(self.stack_layer_estimator.__class__.__name__))\n",
    "        \n",
    "        return self.stack_layer_estimator.predict(self.ensemble_predictions_)\n",
    "\n",
    "    \n",
    "#     # The predict function to make a set of predictions for a set of query instances\n",
    "    def predict_proba(self, X):\n",
    "        \n",
    "        check_is_fitted(self, ['models'])\n",
    "        \n",
    "        self.model_output, self.ensemble_predictions_ = [], []\n",
    "\n",
    "        # Check that the input features match the type and shape of the training features\n",
    "        X = check_array(X)\n",
    "\n",
    "        self.ensemble_predict(X)\n",
    "        \n",
    "        self.prepare_data_for_stack(X)\n",
    "        \n",
    "        if self.verbosity == 2:\n",
    "            print(\"Stack layer with base_estimator {} making prediction\".format(self.stack_layer_estimator.__class__.__name__))\n",
    "       \n",
    "        return(self.stack_layer_estimator.predict_proba(self.ensemble_predictions_))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2,\n",
       "       2, 1, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "n_estimators = 10\n",
    "base_estimator = svm.SVC()\n",
    "hyperparam_range = {\"kernel\":[\"rbf\", \"linear\"], \"C\":np.arange(0.1, 1.0, 0.1), \"gamma\":[0.1, 0.5], \"probability\":[True]}\n",
    "# hyperparam_range = None\n",
    "clf = StackedHeterogenousEnsembleClassifier(base_estimator, n_estimators, hyperparam_range, True, svm.SVC(probability=True), 0.7, verbosity = 0)\n",
    "clf.fit(iris.data, iris.target)\n",
    "clf.predict(iris.data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the StackedHeterogenousEnsembleClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.85473369, 0.08779271, 0.05747361],\n",
       "       [0.85473369, 0.08779271, 0.05747361],\n",
       "       [0.85473369, 0.08779271, 0.05747361],\n",
       "       [0.85473369, 0.08779271, 0.05747361],\n",
       "       [0.85473369, 0.08779271, 0.05747361],\n",
       "       [0.85473369, 0.08779271, 0.05747361],\n",
       "       [0.85473369, 0.08779271, 0.05747361],\n",
       "       [0.85473369, 0.08779271, 0.05747361],\n",
       "       [0.85473369, 0.08779271, 0.05747361],\n",
       "       [0.85473369, 0.08779271, 0.05747361],\n",
       "       [0.85473369, 0.08779271, 0.05747361],\n",
       "       [0.85473369, 0.08779271, 0.05747361],\n",
       "       [0.85473369, 0.08779271, 0.05747361],\n",
       "       [0.85473369, 0.08779271, 0.05747361],\n",
       "       [0.85473369, 0.08779271, 0.05747361],\n",
       "       [0.85473369, 0.08779271, 0.05747361],\n",
       "       [0.85473369, 0.08779271, 0.05747361],\n",
       "       [0.85473369, 0.08779271, 0.05747361],\n",
       "       [0.85473369, 0.08779271, 0.05747361],\n",
       "       [0.85473369, 0.08779271, 0.05747361],\n",
       "       [0.85473369, 0.08779271, 0.05747361],\n",
       "       [0.85473369, 0.08779271, 0.05747361],\n",
       "       [0.85473369, 0.08779271, 0.05747361],\n",
       "       [0.85473369, 0.08779271, 0.05747361],\n",
       "       [0.85473369, 0.08779271, 0.05747361],\n",
       "       [0.85473369, 0.08779271, 0.05747361],\n",
       "       [0.85473369, 0.08779271, 0.05747361],\n",
       "       [0.85473369, 0.08779271, 0.05747361],\n",
       "       [0.85473369, 0.08779271, 0.05747361],\n",
       "       [0.85473369, 0.08779271, 0.05747361],\n",
       "       [0.85473369, 0.08779271, 0.05747361],\n",
       "       [0.85473369, 0.08779271, 0.05747361],\n",
       "       [0.85473369, 0.08779271, 0.05747361],\n",
       "       [0.85473369, 0.08779271, 0.05747361],\n",
       "       [0.85473369, 0.08779271, 0.05747361],\n",
       "       [0.85473369, 0.08779271, 0.05747361],\n",
       "       [0.85473369, 0.08779271, 0.05747361],\n",
       "       [0.85473369, 0.08779271, 0.05747361],\n",
       "       [0.85473369, 0.08779271, 0.05747361],\n",
       "       [0.85473369, 0.08779271, 0.05747361],\n",
       "       [0.85473369, 0.08779271, 0.05747361],\n",
       "       [0.85473369, 0.08779271, 0.05747361],\n",
       "       [0.85473369, 0.08779271, 0.05747361],\n",
       "       [0.85473369, 0.08779271, 0.05747361],\n",
       "       [0.85473369, 0.08779271, 0.05747361],\n",
       "       [0.85473369, 0.08779271, 0.05747361],\n",
       "       [0.85473369, 0.08779271, 0.05747361],\n",
       "       [0.85473369, 0.08779271, 0.05747361],\n",
       "       [0.85473369, 0.08779271, 0.05747361],\n",
       "       [0.85473369, 0.08779271, 0.05747361],\n",
       "       [0.06054199, 0.77212291, 0.16733511],\n",
       "       [0.03465776, 0.92168955, 0.04365269],\n",
       "       [0.05519235, 0.74088075, 0.2039269 ],\n",
       "       [0.03733196, 0.91989134, 0.0427767 ],\n",
       "       [0.0331616 , 0.92295941, 0.043879  ],\n",
       "       [0.03666223, 0.92025495, 0.04308283],\n",
       "       [0.03140009, 0.92458528, 0.04401463],\n",
       "       [0.03733196, 0.91989134, 0.0427767 ],\n",
       "       [0.0331616 , 0.92295941, 0.043879  ],\n",
       "       [0.03733196, 0.91989134, 0.0427767 ],\n",
       "       [0.03733196, 0.91989134, 0.0427767 ],\n",
       "       [0.03666223, 0.92025495, 0.04308283],\n",
       "       [0.03733196, 0.91989134, 0.0427767 ],\n",
       "       [0.03465776, 0.92168955, 0.04365269],\n",
       "       [0.03733196, 0.91989134, 0.0427767 ],\n",
       "       [0.0331616 , 0.92295941, 0.043879  ],\n",
       "       [0.03666223, 0.92025495, 0.04308283],\n",
       "       [0.03733196, 0.91989134, 0.0427767 ],\n",
       "       [0.0359579 , 0.87219052, 0.09185159],\n",
       "       [0.03733196, 0.91989134, 0.0427767 ],\n",
       "       [0.03105506, 0.65925276, 0.30969218],\n",
       "       [0.03733196, 0.91989134, 0.0427767 ],\n",
       "       [0.03105506, 0.65925276, 0.30969218],\n",
       "       [0.03666223, 0.92025495, 0.04308283],\n",
       "       [0.03666223, 0.92025495, 0.04308283],\n",
       "       [0.03465776, 0.92168955, 0.04365269],\n",
       "       [0.0359579 , 0.87219052, 0.09185159],\n",
       "       [0.03034988, 0.07346007, 0.89619005],\n",
       "       [0.03465776, 0.92168955, 0.04365269],\n",
       "       [0.03733196, 0.91989134, 0.0427767 ],\n",
       "       [0.03733196, 0.91989134, 0.0427767 ],\n",
       "       [0.03733196, 0.91989134, 0.0427767 ],\n",
       "       [0.03733196, 0.91989134, 0.0427767 ],\n",
       "       [0.03105506, 0.65925276, 0.30969218],\n",
       "       [0.03666223, 0.92025495, 0.04308283],\n",
       "       [0.03465776, 0.92168955, 0.04365269],\n",
       "       [0.03140009, 0.92458528, 0.04401463],\n",
       "       [0.03666223, 0.92025495, 0.04308283],\n",
       "       [0.03733196, 0.91989134, 0.0427767 ],\n",
       "       [0.03733196, 0.91989134, 0.0427767 ],\n",
       "       [0.03733196, 0.91989134, 0.0427767 ],\n",
       "       [0.03465776, 0.92168955, 0.04365269],\n",
       "       [0.03733196, 0.91989134, 0.0427767 ],\n",
       "       [0.03733196, 0.91989134, 0.0427767 ],\n",
       "       [0.03733196, 0.91989134, 0.0427767 ],\n",
       "       [0.03733196, 0.91989134, 0.0427767 ],\n",
       "       [0.03733196, 0.91989134, 0.0427767 ],\n",
       "       [0.03666223, 0.92025495, 0.04308283],\n",
       "       [0.03733196, 0.91989134, 0.0427767 ],\n",
       "       [0.03733196, 0.91989134, 0.0427767 ],\n",
       "       [0.03034988, 0.07346007, 0.89619005],\n",
       "       [0.03034988, 0.07346007, 0.89619005],\n",
       "       [0.03034988, 0.07346007, 0.89619005],\n",
       "       [0.03034988, 0.07346007, 0.89619005],\n",
       "       [0.03034988, 0.07346007, 0.89619005],\n",
       "       [0.03034988, 0.07346007, 0.89619005],\n",
       "       [0.0594463 , 0.78743771, 0.15311598],\n",
       "       [0.03034988, 0.07346007, 0.89619005],\n",
       "       [0.03034988, 0.07346007, 0.89619005],\n",
       "       [0.03034988, 0.07346007, 0.89619005],\n",
       "       [0.03034988, 0.07346007, 0.89619005],\n",
       "       [0.03034988, 0.07346007, 0.89619005],\n",
       "       [0.03034988, 0.07346007, 0.89619005],\n",
       "       [0.02829864, 0.16989628, 0.80180508],\n",
       "       [0.03034988, 0.07346007, 0.89619005],\n",
       "       [0.03034988, 0.07346007, 0.89619005],\n",
       "       [0.03034988, 0.07346007, 0.89619005],\n",
       "       [0.03034988, 0.07346007, 0.89619005],\n",
       "       [0.03034988, 0.07346007, 0.89619005],\n",
       "       [0.03105506, 0.65925276, 0.30969218],\n",
       "       [0.03034988, 0.07346007, 0.89619005],\n",
       "       [0.02829864, 0.16989628, 0.80180508],\n",
       "       [0.03034988, 0.07346007, 0.89619005],\n",
       "       [0.03034988, 0.07346007, 0.89619005],\n",
       "       [0.03034988, 0.07346007, 0.89619005],\n",
       "       [0.03034988, 0.07346007, 0.89619005],\n",
       "       [0.03105506, 0.65925276, 0.30969218],\n",
       "       [0.02829864, 0.16989628, 0.80180508],\n",
       "       [0.03034988, 0.07346007, 0.89619005],\n",
       "       [0.03034988, 0.07346007, 0.89619005],\n",
       "       [0.03034988, 0.07346007, 0.89619005],\n",
       "       [0.03034988, 0.07346007, 0.89619005],\n",
       "       [0.03034988, 0.07346007, 0.89619005],\n",
       "       [0.03105506, 0.65925276, 0.30969218],\n",
       "       [0.03034988, 0.07346007, 0.89619005],\n",
       "       [0.03034988, 0.07346007, 0.89619005],\n",
       "       [0.03034988, 0.07346007, 0.89619005],\n",
       "       [0.03034988, 0.07346007, 0.89619005],\n",
       "       [0.03105506, 0.65925276, 0.30969218],\n",
       "       [0.03034988, 0.07346007, 0.89619005],\n",
       "       [0.03034988, 0.07346007, 0.89619005],\n",
       "       [0.03034988, 0.07346007, 0.89619005],\n",
       "       [0.03034988, 0.07346007, 0.89619005],\n",
       "       [0.03034988, 0.07346007, 0.89619005],\n",
       "       [0.03034988, 0.07346007, 0.89619005],\n",
       "       [0.03034988, 0.07346007, 0.89619005],\n",
       "       [0.03034988, 0.07346007, 0.89619005],\n",
       "       [0.03034988, 0.07346007, 0.89619005],\n",
       "       [0.03034988, 0.07346007, 0.89619005],\n",
       "       [0.03034988, 0.07346007, 0.89619005]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict_proba(iris.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform a simple test using the StackedHeterogenousEnsembleClassifier on the Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train of size 600 split at 0.7 ratio\n",
      "10 models fitted with base estimator SVC.\n",
      "Training set of size 1800 prepared for stack layer\n",
      "Stack layer with base estimator SVC has been fit\n",
      "Predict completed for 10 models \n",
      "Preparing data for stack layer.\n",
      "Data set of size 1500 prepared for stack layer\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        50\n",
      "           1       0.88      1.00      0.93        50\n",
      "           2       1.00      0.86      0.92        50\n",
      "\n",
      "    accuracy                           0.95       150\n",
      "   macro avg       0.96      0.95      0.95       150\n",
      "weighted avg       0.96      0.95      0.95       150\n",
      "\n",
      "Confusion Matrix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>43</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>50</td>\n",
       "      <td>57</td>\n",
       "      <td>43</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted   0   1   2  All\n",
       "True                      \n",
       "0          50   0   0   50\n",
       "1           0  50   0   50\n",
       "2           0   7  43   50\n",
       "All        50  57  43  150"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict completed for 10 models \n",
      "Preparing data for stack layer.\n",
      "Data set of size 1500 prepared for stack layer\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.89379542, 0.05281921, 0.05338537],\n",
       "       [0.89379542, 0.05281921, 0.05338537],\n",
       "       [0.89379542, 0.05281921, 0.05338537],\n",
       "       [0.89379542, 0.05281921, 0.05338537],\n",
       "       [0.89379542, 0.05281921, 0.05338537],\n",
       "       [0.89379542, 0.05281921, 0.05338537],\n",
       "       [0.89379542, 0.05281921, 0.05338537],\n",
       "       [0.89379542, 0.05281921, 0.05338537],\n",
       "       [0.89379542, 0.05281921, 0.05338537],\n",
       "       [0.89379542, 0.05281921, 0.05338537]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "n_estimators = 10\n",
    "base_estimator = svm.SVC()\n",
    "hyperparam_range = {\"kernel\":[\"rbf\", \"linear\"], \"C\":np.arange(0.1, 1.0, 0.1), \"gamma\":[0.1, 0.5], \"probability\":[True]}\n",
    "clf = StackedHeterogenousEnsembleClassifier(base_estimator, n_estimators, hyperparam_range, True, svm.SVC(probability=True), 0.7, verbosity = 1)\n",
    "clf.fit(iris.data, iris.target)\n",
    "y_pred = clf.predict(iris.data)\n",
    "print(metrics.classification_report(iris.target, y_pred))\n",
    "print(\"Confusion Matrix\")\n",
    "display(pd.crosstab(np.array(iris.target), y_pred, rownames=['True'], colnames=['Predicted'], margins=True))\n",
    "y_pred = clf.predict_proba(iris.data)\n",
    "y_pred[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform a cross validation experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train of size 540 split at 0.7 ratio\n",
      "10 models fitted with base estimator SVC.\n",
      "Training set of size 1640 prepared for stack layer\n",
      "Stack layer with base estimator SVC has been fit\n",
      "Predict completed for 10 models \n",
      "Preparing data for stack layer.\n",
      "Data set of size 150 prepared for stack layer\n",
      "X_train of size 540 split at 0.7 ratio\n",
      "10 models fitted with base estimator SVC.\n",
      "Training set of size 1640 prepared for stack layer\n",
      "Stack layer with base estimator SVC has been fit\n",
      "Predict completed for 10 models \n",
      "Preparing data for stack layer.\n",
      "Data set of size 150 prepared for stack layer\n",
      "X_train of size 540 split at 0.7 ratio\n",
      "10 models fitted with base estimator SVC.\n",
      "Training set of size 1640 prepared for stack layer\n",
      "Stack layer with base estimator SVC has been fit\n",
      "Predict completed for 10 models \n",
      "Preparing data for stack layer.\n",
      "Data set of size 150 prepared for stack layer\n",
      "X_train of size 540 split at 0.7 ratio\n",
      "10 models fitted with base estimator SVC.\n",
      "Training set of size 1640 prepared for stack layer\n",
      "Stack layer with base estimator SVC has been fit\n",
      "Predict completed for 10 models \n",
      "Preparing data for stack layer.\n",
      "Data set of size 150 prepared for stack layer\n",
      "X_train of size 540 split at 0.7 ratio\n",
      "10 models fitted with base estimator SVC.\n",
      "Training set of size 1640 prepared for stack layer\n",
      "Stack layer with base estimator SVC has been fit\n",
      "Predict completed for 10 models \n",
      "Preparing data for stack layer.\n",
      "Data set of size 150 prepared for stack layer\n",
      "X_train of size 540 split at 0.7 ratio\n",
      "10 models fitted with base estimator SVC.\n",
      "Training set of size 1640 prepared for stack layer\n",
      "Stack layer with base estimator SVC has been fit\n",
      "Predict completed for 10 models \n",
      "Preparing data for stack layer.\n",
      "Data set of size 150 prepared for stack layer\n",
      "X_train of size 540 split at 0.7 ratio\n",
      "10 models fitted with base estimator SVC.\n",
      "Training set of size 1640 prepared for stack layer\n",
      "Stack layer with base estimator SVC has been fit\n",
      "Predict completed for 10 models \n",
      "Preparing data for stack layer.\n",
      "Data set of size 150 prepared for stack layer\n",
      "X_train of size 540 split at 0.7 ratio\n",
      "10 models fitted with base estimator SVC.\n",
      "Training set of size 1640 prepared for stack layer\n",
      "Stack layer with base estimator SVC has been fit\n",
      "Predict completed for 10 models \n",
      "Preparing data for stack layer.\n",
      "Data set of size 150 prepared for stack layer\n",
      "X_train of size 540 split at 0.7 ratio\n",
      "10 models fitted with base estimator SVC.\n",
      "Training set of size 1640 prepared for stack layer\n",
      "Stack layer with base estimator SVC has been fit\n",
      "Predict completed for 10 models \n",
      "Preparing data for stack layer.\n",
      "Data set of size 150 prepared for stack layer\n",
      "X_train of size 540 split at 0.7 ratio\n",
      "10 models fitted with base estimator SVC.\n",
      "Training set of size 1640 prepared for stack layer\n",
      "Stack layer with base estimator SVC has been fit\n",
      "Predict completed for 10 models \n",
      "Preparing data for stack layer.\n",
      "Data set of size 150 prepared for stack layer\n",
      "[0.93333333 0.93333333 0.93333333 0.93333333 0.93333333 1.\n",
      " 0.93333333 1.         1.         1.        ]\n",
      "0.96  +/-  0.03265986323710903\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(clf, iris.data, iris.target, cv=10)\n",
    "print(scores)\n",
    "print(np.mean(scores), \" +/- \", np.std(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Compare the Performance of the Different Ensembles Defined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Experiment Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take only a sample of the dataset for fast testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sampling_rate = .2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset and explore it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F1</th>\n",
       "      <th>F2</th>\n",
       "      <th>F3</th>\n",
       "      <th>F4</th>\n",
       "      <th>F5</th>\n",
       "      <th>F6</th>\n",
       "      <th>F7</th>\n",
       "      <th>F8</th>\n",
       "      <th>F9</th>\n",
       "      <th>F10</th>\n",
       "      <th>...</th>\n",
       "      <th>F40</th>\n",
       "      <th>F41</th>\n",
       "      <th>F42</th>\n",
       "      <th>F43</th>\n",
       "      <th>F44</th>\n",
       "      <th>F45</th>\n",
       "      <th>F46</th>\n",
       "      <th>F47</th>\n",
       "      <th>F48</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21304</th>\n",
       "      <td>3.270000e-07</td>\n",
       "      <td>4.260000e-08</td>\n",
       "      <td>-3.140000e-08</td>\n",
       "      <td>1.570000e-06</td>\n",
       "      <td>-2.800000e-08</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>0.021835</td>\n",
       "      <td>0.021835</td>\n",
       "      <td>0.021835</td>\n",
       "      <td>0.014851</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.59629</td>\n",
       "      <td>37.19400</td>\n",
       "      <td>7.4664</td>\n",
       "      <td>-1.4966</td>\n",
       "      <td>-1.4966</td>\n",
       "      <td>-1.4966</td>\n",
       "      <td>-1.4978</td>\n",
       "      <td>-1.4978</td>\n",
       "      <td>-1.4978</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2987</th>\n",
       "      <td>1.090000e-05</td>\n",
       "      <td>7.000000e-05</td>\n",
       "      <td>-1.074400e-04</td>\n",
       "      <td>7.860000e-06</td>\n",
       "      <td>-2.680000e-05</td>\n",
       "      <td>-0.000261</td>\n",
       "      <td>0.022892</td>\n",
       "      <td>0.022822</td>\n",
       "      <td>0.022930</td>\n",
       "      <td>-0.025140</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.70969</td>\n",
       "      <td>2.69210</td>\n",
       "      <td>4.5057</td>\n",
       "      <td>-1.5031</td>\n",
       "      <td>-1.5031</td>\n",
       "      <td>-1.5028</td>\n",
       "      <td>-1.4940</td>\n",
       "      <td>-1.4942</td>\n",
       "      <td>-1.4941</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35445</th>\n",
       "      <td>8.180000e-06</td>\n",
       "      <td>-1.260000e-05</td>\n",
       "      <td>-2.797200e-04</td>\n",
       "      <td>1.360000e-06</td>\n",
       "      <td>-2.150000e-06</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>0.027289</td>\n",
       "      <td>0.027301</td>\n",
       "      <td>0.027581</td>\n",
       "      <td>0.082276</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.69002</td>\n",
       "      <td>0.52407</td>\n",
       "      <td>2.9439</td>\n",
       "      <td>-1.4982</td>\n",
       "      <td>-1.4983</td>\n",
       "      <td>-1.4983</td>\n",
       "      <td>-1.4974</td>\n",
       "      <td>-1.4974</td>\n",
       "      <td>-1.4974</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22940</th>\n",
       "      <td>2.200000e-06</td>\n",
       "      <td>-1.760000e-05</td>\n",
       "      <td>2.590000e-05</td>\n",
       "      <td>6.500000e-06</td>\n",
       "      <td>-1.670000e-05</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.016400</td>\n",
       "      <td>0.016418</td>\n",
       "      <td>0.016392</td>\n",
       "      <td>0.011953</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.72872</td>\n",
       "      <td>8.35150</td>\n",
       "      <td>5.6663</td>\n",
       "      <td>-1.4997</td>\n",
       "      <td>-1.4997</td>\n",
       "      <td>-1.4997</td>\n",
       "      <td>-1.4978</td>\n",
       "      <td>-1.4978</td>\n",
       "      <td>-1.4977</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17399</th>\n",
       "      <td>-9.870000e-07</td>\n",
       "      <td>2.220000e-05</td>\n",
       "      <td>6.830000e-05</td>\n",
       "      <td>1.480000e-07</td>\n",
       "      <td>8.470000e-06</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>0.016726</td>\n",
       "      <td>0.016704</td>\n",
       "      <td>0.016636</td>\n",
       "      <td>0.045063</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.77271</td>\n",
       "      <td>4.69450</td>\n",
       "      <td>22.4840</td>\n",
       "      <td>-1.4992</td>\n",
       "      <td>-1.4992</td>\n",
       "      <td>-1.4992</td>\n",
       "      <td>-1.4998</td>\n",
       "      <td>-1.4999</td>\n",
       "      <td>-1.4999</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 F1            F2            F3            F4            F5  \\\n",
       "21304  3.270000e-07  4.260000e-08 -3.140000e-08  1.570000e-06 -2.800000e-08   \n",
       "2987   1.090000e-05  7.000000e-05 -1.074400e-04  7.860000e-06 -2.680000e-05   \n",
       "35445  8.180000e-06 -1.260000e-05 -2.797200e-04  1.360000e-06 -2.150000e-06   \n",
       "22940  2.200000e-06 -1.760000e-05  2.590000e-05  6.500000e-06 -1.670000e-05   \n",
       "17399 -9.870000e-07  2.220000e-05  6.830000e-05  1.480000e-07  8.470000e-06   \n",
       "\n",
       "             F6        F7        F8        F9       F10  ...      F40  \\\n",
       "21304 -0.000005  0.021835  0.021835  0.021835  0.014851  ... -0.59629   \n",
       "2987  -0.000261  0.022892  0.022822  0.022930 -0.025140  ... -0.70969   \n",
       "35445  0.000176  0.027289  0.027301  0.027581  0.082276  ... -0.69002   \n",
       "22940  0.000015  0.016400  0.016418  0.016392  0.011953  ... -0.72872   \n",
       "17399 -0.000024  0.016726  0.016704  0.016636  0.045063  ... -0.77271   \n",
       "\n",
       "            F41      F42     F43     F44     F45     F46     F47     F48  \\\n",
       "21304  37.19400   7.4664 -1.4966 -1.4966 -1.4966 -1.4978 -1.4978 -1.4978   \n",
       "2987    2.69210   4.5057 -1.5031 -1.5031 -1.5028 -1.4940 -1.4942 -1.4941   \n",
       "35445   0.52407   2.9439 -1.4982 -1.4983 -1.4983 -1.4974 -1.4974 -1.4974   \n",
       "22940   8.35150   5.6663 -1.4997 -1.4997 -1.4997 -1.4978 -1.4978 -1.4977   \n",
       "17399   4.69450  22.4840 -1.4992 -1.4992 -1.4992 -1.4998 -1.4999 -1.4999   \n",
       "\n",
       "       label  \n",
       "21304      5  \n",
       "2987       1  \n",
       "35445      7  \n",
       "22940      5  \n",
       "17399      4  \n",
       "\n",
       "[5 rows x 49 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = pd.read_csv('Sensorless_drive_diagnosis.csv')\n",
    "dataset = dataset.sample(frac=data_sampling_rate) #take a sample from the dataset so everyhting runs smoothly\n",
    "display(dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(\"Missing Values\")\n",
    "print(sum(dataset.isnull().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9     0.093916\n",
       "7     0.093745\n",
       "11    0.092976\n",
       "4     0.092377\n",
       "10    0.090839\n",
       "2     0.090668\n",
       "8     0.090412\n",
       "6     0.089899\n",
       "5     0.089899\n",
       "1     0.089814\n",
       "3     0.085455\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = dataset.pop('label')\n",
    "X = dataset\n",
    "y.value_counts()/len(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5850, 48) (3511, 48) (2341, 48)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test \\\n",
    "    = train_test_split(X, y, \\\n",
    "                       shuffle=True, \\\n",
    "                       stratify = y, \\\n",
    "                       train_size = 0.7)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid \\\n",
    "    = train_test_split(X_train, y_train, \\\n",
    "                        shuffle=True, \\\n",
    "                        stratify = y_train, \\\n",
    "                        train_size = 0.5/0.7)\n",
    "print(X_train.shape, X_test.shape, X_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the min max scalar object\n",
    "min_max_scaler = preprocessing.MinMaxScaler((-1,1))\n",
    "min_max_scaler.fit(X_train)\n",
    "\n",
    "# Train the scalar on the training dataset\n",
    "a = min_max_scaler.transform(X_train)\n",
    "\n",
    "# Little trick to stop transform from pandas daataframe to numpy array losing column namesWatch out for putting back in columns here\n",
    "cols = X_train.columns\n",
    "X_train = pd.DataFrame(a, columns = cols) \n",
    "\n",
    "# Also normalise other partitions\n",
    "a = min_max_scaler.transform(X_valid)\n",
    "X_valid = pd.DataFrame(a, columns = cols) \n",
    "a = min_max_scaler.transform(X_test)\n",
    "X_test = pd.DataFrame(a, columns = cols) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to develop an initial performance baseline , I decided to compare the performance of the Heterogeneous Ensemble against itself, given four different base classifiers. \n",
    "- I decided to use Decision Trees ars they often work well in ensembles. - I then selected kNN as it often does not perform well in ensembles, due to its' stability, and though it would be interesting to see if it performed better with a Heterogeneous Ensemble. \n",
    "- SVM is the default classifier given, and hence worth considering in testing. It is also already an ensemble, so it will be interesting to see if the additional diversity produced by the sampling of the hyper paramter space has a significant impact.  \n",
    "- Finally, I decided to use a logistic regressor, as I have not seen logistic regression frequently used in ensembles, and I was curious as to its' performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from collections import Counter\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import plot_precision_recall_curve\n",
    "from sklearn.ensemble import BaggingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}\n",
    "models['DTree'] = DecisionTreeClassifier(random_state=42)\n",
    "models['kNN'] = KNeighborsClassifier()  \n",
    "models['SVM'] = svm.SVC(random_state=42)\n",
    "models['LR'] = LogisticRegression(random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "##11 hyper parameters each "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {}\n",
    "hparams['DTree'] = {'max_depth': list(range(2,100)), 'min_samples_split': list(range(2,50))}\n",
    "hparams['kNN'] = {'n_neighbors': list(range(3,50)), 'metric': ['euclidean', 'chebyshev', 'minkowski', 'manhattan']}\n",
    "hparams['SVM'] = {'kernel': ['linear', 'poly', 'rbf', 'sigmoid'], 'gamma': np.arange(0.1, 10, .1).tolist()}\n",
    "hparams['LR'] = {'C': np.arange(1.0, 10.0, 0.5).tolist(), 'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this initial test we will be comparing the performance of the ensemble of each classifier against the single classifier itself, and also against a bagged version of the classifier. As an initial measure of performance, we will be looking at the accuracy score when using the valdation set. The parameter ranges have been hand-selected with the goal of provoking the most diversity possible for the heterogenous ensemble. For this initial test, n will be left at the deafult, 10, for both the bootstrapped ensemble and the heterogeneous ensemble. For both the bootstrapped classifier, the default parameters will be used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_1 = pd.DataFrame(columns=[m for m in hparams.keys()],\n",
    "                            index = ['Single', 'Bootstrapped', 'Ensemble', 'Stacked Ensemble'])\n",
    "benchmark_2 = pd.DataFrame(columns=[m for m in hparams.keys()],\n",
    "                            index = ['Single', 'Bootstrapped', 'Ensemble', 'Stacked Ensemble'])\n",
    "for m in models:\n",
    "    clfs = [copy(models[m]),BaggingClassifier(base_estimator=copy(models[m]), random_state=42),\n",
    "           HeterogenousEnsembleClassifier(base_estimator=copy(models[m]), hp_range_map=hparams[m],random_state=42),\n",
    "           StackedHeterogenousEnsembleClassifier(base_estimator=copy(models[m]), hp_range_map=hparams[m],random_state=42)]\n",
    "    pred = []\n",
    "    f1 = []\n",
    "    for c in clfs:\n",
    "        clf = c\n",
    "        y_preds = clf.fit(X_train, y_train).predict(X_valid)\n",
    "        pred.append(accuracy_score(y_valid, y_preds))\n",
    "        f1.append(f1_score(y_valid, y_preds, average='macro'))\n",
    "    benchmark_1[m] = pred\n",
    "    benchmark_2[m] = f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked_benchmark = benchmark_1.copy().rank(ascending=False)\n",
    "ranked_benchmark['average'] = ranked_benchmark.mean(axis=1).rank()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked_benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we can see that the bootstrapped clasifiers and the heterogeneous ensemble seem to be perfoming best in terms of the rankings. These rankings do not give us a view on the actual range of performance with regards to accuracy. Below I will plot he accuracy scores to give us a clearer view of their performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked_benchmark2 = benchmark_2.copy().rank(ascending=False)\n",
    "ranked_benchmark2['average'] = ranked_benchmark2.mean(axis=1).rank()\n",
    "ranked_benchmark2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see here that the rankings for the F1 score of the models are identical to the accuracy rankings. Given that the data is relatively balanced, this makes sense, the models are behaving in a fairly unbiased mannor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_subplot(fig, ax, title, res):\n",
    "    X = np.arange(1)\n",
    "    ax.axhline(y=res.iloc[0], color='black')\n",
    "    ax.bar(X + 0.00, res.iloc[0], color = 'b', alpha=0.5, width = 0.05, label=\"Single\")\n",
    "    ax.bar(X + 0.10, res.iloc[1], color = 'y', alpha=0.5, width = 0.05, label = \"Bootstrapped\")\n",
    "    ax.bar(X + 0.20, res.iloc[2], color = 'g', alpha=0.5, width = 0.05, label = \"Heterogeneous\")\n",
    "    ax.bar(X + 0.30, res.iloc[3], color = 'r', alpha=0.5, width = 0.05, label = \"Heterogeneous\")\n",
    "    ax.set_title(title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axs = plt.subplots(2, 2, figsize=(14,12), sharex=True)\n",
    "plot_subplot(f, axs[0,0], \"Decision Tree\", benchmark_1['DTree'])\n",
    "plot_subplot(f, axs[0,1], \"kNN\", benchmark_1['kNN'])\n",
    "plot_subplot(f, axs[1,0], \"SVM\", benchmark_1['SVM'])\n",
    "plot_subplot(f, axs[1,1], \"Logistic Regression\", benchmark_1['LR'])\n",
    "b = mpatches.Patch(color='blue', alpha=0.5, label='Single')\n",
    "y = mpatches.Patch(color='yellow', alpha=0.5, label='Bootstrapped')\n",
    "g = mpatches.Patch(color='green', alpha=0.5, label='Heterogeneous')\n",
    "r = mpatches.Patch(color='red', alpha=0.5, label='Stacked Heterogeneous')\n",
    "plt.legend(handles=[b,y,g,r], bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "f.suptitle(\"Accuracy Scores\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Globally the rankings of performance are very changeable. Although the non-stacked heterogeneous ensemble is first overall in the average rankings, there is no obvious \"winner\". \n",
    "- For the decision tree, the stacked classifier has performed the worst, even worse than the single base classifier. The heterogeneous ensemble has come in second to the boosted classifier. \n",
    "- For kNN, the heterogeneous ensemble has perforformed the worst on this data, and the stacked hetereogeneous ensemble has performed the best. This is an interesting result, clearly the stacked classifier has learnt something more sophisticated than predicting the majority in this instance.\n",
    "- For SVM both the heterogeneous ensemble and the stacked heterogeneous ensemble have performed very well, with the stacked heterogeneous ensemble performing slightly worse than the standard heterogeneous ensemble. \n",
    "- And for Logistic Regression we see a similar result, the two heterogeneous ensembles have performed the best, with the non-stacked model performing slightly better than the stacked model.\n",
    "\n",
    "Given that holdout testing is done with a single \"slice\" of the data, it hard to discern whether these results are fiable our not. I have decided to take the models where the stacked classifier performed the worst (Decision tree) and the non-stacked classifier performed the worst (kNN) and cross validate, while also varying the random seed, to check the stability of these results. I am leaving the hyper paramater range unchanged.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate, KFold\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reps = 5\n",
    "folds = 10\n",
    "benchmark_3 = pd.DataFrame(index = ['Single', 'Bootstrapped', 'Ensemble', 'Stacked Ensemble'],\n",
    "                           columns = ['Fold ' + str(i) for i in range(1,reps+1)])\n",
    "for i in range (0, reps):\n",
    "    kf = KFold(n_splits = folds, shuffle = True)\n",
    "    clfs = [DecisionTreeClassifier(random_state =i+i),\n",
    "            BaggingClassifier(base_estimator=copy(DecisionTreeClassifier(random_state = i+i)), random_state=i+i),\n",
    "            HeterogenousEnsembleClassifier(base_estimator=DecisionTreeClassifier(random_state =i+i), hp_range_map=hparams['DTree'],random_state=i+i, verbosity=0),\n",
    "            StackedHeterogenousEnsembleClassifier(base_estimator=DecisionTreeClassifier(random_state =i+i), hp_range_map=hparams['DTree'],random_state=i+i, verbosity=0)]\n",
    "    results = []\n",
    "    for c in clfs:\n",
    "        print(\"rep \", i+1,\"class \", c.__class__.__name__)\n",
    "        xval = cross_val_score(c, X_train, y_train, cv=kf, scoring='accuracy', error_score=\"raise\")\n",
    "        results.append(xval.mean())\n",
    "    benchmark_3['Fold '+str(i+1)] = results\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked_benchmark3 = benchmark_3.copy().rank(ascending=False)\n",
    "ranked_benchmark3['average'] = ranked_benchmark3.mean(axis=1).rank()\n",
    "ranked_benchmark3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results here are extremely stable. For 5 repitions of 10 fold cross validation, with a different random state seed and shuffling of the data at each repition, the ranks have not varied. The stacked ensemble definitively does not perform well with a Decision Tree as a base classifier. This is very unusual, given that decision trees ususally respond well to ensembles. It is possible that the number of estimators is not sufficient, but given that the non-stacked heterogeneous ensemble is performing relatively well, that seems unlikely. It may be the combination of Decision Tree with SVM as the stack, that is not combining well together. We will look at this later in the experiment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reps = 5\n",
    "folds = 10\n",
    "benchmark_4 = pd.DataFrame(index = ['Single', 'Bootstrapped', 'Ensemble', 'Stacked Ensemble'],\n",
    "                           columns = ['Fold ' + str(i) for i in range(1,reps+1)])\n",
    "for i in range (0, reps):\n",
    "    kf = KFold(n_splits = folds, shuffle = True)\n",
    "    clfs = [ KNeighborsClassifier() ,\n",
    "            BaggingClassifier(base_estimator=KNeighborsClassifier(), random_state=i+i),\n",
    "            HeterogenousEnsembleClassifier(base_estimator= KNeighborsClassifier(), hp_range_map=hparams['kNN'],random_state=i+i, verbosity=0),\n",
    "            StackedHeterogenousEnsembleClassifier(base_estimator= KNeighborsClassifier() , hp_range_map=hparams['kNN'],random_state=i+i, verbosity=0)]\n",
    "    results = []\n",
    "    for c in clfs:\n",
    "        print(\"rep \", i+1,\"class \", c.__class__.__name__)\n",
    "        xval = cross_val_score(c, X_train, y_train, cv=kf, scoring='accuracy', error_score=\"raise\")\n",
    "        results.append(xval.mean())\n",
    "    benchmark_4['Fold '+str(i+1)] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked_benchmark4 = benchmark_4.copy().rank(ascending=False)\n",
    "ranked_benchmark4['average'] = ranked_benchmark4.mean(axis=1).rank()\n",
    "ranked_benchmark4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see much more unstable results. In holdout testing our stacked ensemble had the best performance, here that performance is only repeated on Fold 5. On average, the bootstrapped classifier has performed best, and the stacked has performed slightly better than the non-stacked ensemble. In general kNN is not known to respond well to ensembles, normally subspace bootstrapping needs to be performed to generate enough diversity for a real performance gain to be seen. So, it is unsurprising that we see the Single classifier performing as well as it does. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the testing that follows, I will be using SVM as our base classifier as both the stacked and non stacked ensemble performed well with this base classifier, as seen below. As SVM is much more computationally heavy to run, I have reduced the fold size to 5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reps = 5\n",
    "folds = 5\n",
    "benchmark_5 = pd.DataFrame(index = ['Single', 'Bootstrapped', 'Ensemble', 'Stacked Ensemble'],\n",
    "                           columns = ['Fold ' + str(i) for i in range(1,reps+1)])\n",
    "for i in range (0, reps):\n",
    "    kf = KFold(n_splits = folds, shuffle = True)\n",
    "    clfs = [svm.SVC(random_state=i+i) ,\n",
    "            BaggingClassifier(base_estimator=svm.SVC(random_state=i+i), random_state=i+i),\n",
    "            HeterogenousEnsembleClassifier(base_estimator= svm.SVC(random_state=i+i), hp_range_map=hparams['SVM'],random_state=i+i, verbosity=0),\n",
    "            StackedHeterogenousEnsembleClassifier(base_estimator= svm.SVC(random_state=i+i) , hp_range_map=hparams['SVM'],random_state=i+i, verbosity=0)]\n",
    "    results = []\n",
    "    for c in clfs:\n",
    "        print(\"rep \", i+1,\"class \", c.__class__.__name__)\n",
    "        xval = cross_val_score(c, X_train, y_train, cv=kf, scoring='accuracy', error_score=\"raise\")\n",
    "        results.append(xval.mean())\n",
    "    benchmark_5['Fold '+str(i+1)] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked_benchmark5 = benchmark_5.copy().rank(ascending=False)\n",
    "ranked_benchmark5['average'] = ranked_benchmark5.mean(axis=1).rank()\n",
    "ranked_benchmark5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that although the non-stacked ensemble has the best overall ranking, first and second place in the rankings tends to alternate between the stacked and non-stacked heterogeneous classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next I would like to consider the effect of N-number on accuracy. Here we will again use a holdout set instead of cross validation, and see how the classifier performs both in terms of accuracy and F1 score. As a base line we will compare it in accuracy to the performance of a a single base classifer, and a bootstrapped classifier. The random state will not be varied, but will be uniform for all classifiers (42). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC(random_state=42).fit(X_train, y_train)\n",
    "clf2 = BaggingClassifier(base_estimator=clf, random_state=42).fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_valid)\n",
    "sv_base_acc = accuracy_score(y_pred, y_valid)\n",
    "sv_base_f1 = f1_score(y_pred, y_valid, average='macro')\n",
    "y_pred = clf2.predict(X_valid)\n",
    "sv_boost_acc = accuracy_score(y_pred, y_valid)\n",
    "sv_boost_f1 = f1_score(y_pred, y_valid, average='macro')\n",
    "\n",
    "sv_acc = []\n",
    "sv_f1 = []\n",
    "sv_stack_acc = []\n",
    "sv_stack_f1 = []\n",
    "n_range = [5, 10, 20, 30, 50, 100]\n",
    "for n in n_range:\n",
    "    print('n = ', n)\n",
    "    clf3 = HeterogenousEnsembleClassifier(clf, hp_range_map=hparams['SVM'], n_estimators=n, random_state=42)\n",
    "    clf3.fit(X_train, y_train)\n",
    "    y_pred = clf3.predict(X_valid)\n",
    "    sv_acc.append(accuracy_score(y_pred, y_valid))\n",
    "    sv_f1.append(f1_score(y_pred, y_valid, average='macro'))\n",
    "    clf4 = StackedHeterogenousEnsembleClassifier(base_estimator=clf, hp_range_map=hparams['SVM'], n_estimators=n, random_state=42)\n",
    "    clf4.fit(X_train, y_train)\n",
    "    y_pred = clf4.predict(X_valid)\n",
    "    sv_stack_acc.append(accuracy_score(y_pred, y_valid))\n",
    "    sv_stack_f1.append(f1_score(y_pred, y_valid, average='macro'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax1, ax2) = plt.subplots(2, 1, figsize=(12,12))\n",
    "x = list(range(0,len(n_range)))\n",
    "ax1.axhline(y=sv_base_acc, color='red', label='Base Estimator')\n",
    "ax1.axhline(y=sv_boost_acc, color='orange', label='Boosted Estimator')\n",
    "ax1.plot(x, sv_stack_acc, label=\"StackedHeterogeneousEnsemble\", color=\"blue\")\n",
    "ax1.plot(x, sv_acc, label=\"HeterogeneousEnsemble\", color=\"green\")\n",
    "ax1.legend(loc='upper left')\n",
    "ax1.set_ylabel(\"Accuracy\")\n",
    "ax1.set_title(\"Accuracy Scores\")\n",
    "ax1.set_xticklabels([0] + n_range)\n",
    "ax1.set_xlabel(\"N estimators\")\n",
    "\n",
    "ax2.axhline(y=sv_base_f1, color='red', label='Base Estimator')\n",
    "ax2.axhline(y=sv_boost_f1, color='orange', label='Boosted Estimator')\n",
    "ax2.plot(x, sv_stack_f1, label=\"StackedHeterogeneousEnsemble\", color=\"blue\")\n",
    "ax2.plot(x, sv_f1, label=\"HeterogeneousEnsemble\", color=\"green\")\n",
    "ax2.legend(loc='upper left')\n",
    "ax2.set_ylabel(\"F1 Score\")\n",
    "ax2.set_xlabel(\"n_estimators\")\n",
    "ax2.set_xticklabels([0] + n_range)\n",
    "\n",
    "f.set_xticklabels = n_range\n",
    "ax2.set_title(\"F1 Scores\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that accuracy does seem to increase as n_estimators increases for the heterogeneous ensemble, but for the stacked model it flattens out after 20. One possible reason for this is the curse of dimensionality. For n = 100, for instance, the stacked estimator will be receiving each query with (n * number_of_classes), in this case 1000 attributes per query. It does make me question the implementation of my stacked estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am curious if this drop off in accuracy will be visible no matter the base classifier for the stacked model. I decided to next investigate performance of differing base estimators for the stack. For a baseline, I will compare performance against the non-heterogeneous classifier. N will be set to 30, as that is where accuracy started to diverge in the previous experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = HeterogenousEnsembleClassifier(svm.SVC(random_state=0), hp_range_map=hparams['SVM'], n_estimators=30, random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_valid)\n",
    "base_acc_30 = accuracy_score(y_pred, y_valid)\n",
    "clf = HeterogenousEnsembleClassifier(svm.SVC(random_state=0), hp_range_map=hparams['SVM'], n_estimators=10, random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_valid)\n",
    "base_acc_10 = accuracy_score(y_pred, y_valid)\n",
    "acc = []\n",
    "acc2 = []\n",
    "for m in models:\n",
    "    print(m)\n",
    "    clf = StackedHeterogenousEnsembleClassifier(base_estimator=svm.SVC(random_state=42), hp_range_map=hparams['SVM'], n_estimators=30, random_state=0, stack_layer_estimator=copy(models[m]))\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_valid)\n",
    "    acc.append(accuracy_score(y_pred, y_valid))\n",
    "    clf = StackedHeterogenousEnsembleClassifier(base_estimator=svm.SVC(random_state=42), hp_range_map=hparams['SVM'], n_estimators=10, random_state=0, stack_layer_estimator=copy(models[m]))\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_valid)\n",
    "    acc2.append(accuracy_score(y_pred, y_valid))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,12), sharey=True)\n",
    "x = list(range(0,len(models)))\n",
    "ax1.plot(x, acc, label=\"StackedHeterogeneousEnsemble\", color=\"orange\")\n",
    "ax1.axhline(y=base_acc_30, color='red', label='HeterogeneousEnsemble')\n",
    "ax1.legend(loc='lower right')\n",
    "ax1.set_title(\"N=30\")\n",
    "ax1.set_xticks(list(range(0, len(models))))\n",
    "ax1.set_xticklabels([m for m in models])\n",
    "ax1.set_xlabel(\"Accuracy\")\n",
    "ax1.set_ylabel(\"Stack estimator\")\n",
    "\n",
    "ax2.plot(x, acc2, label=\"StackedHeterogeneousEnsemble\", color=\"orange\")\n",
    "ax2.axhline(y=base_acc_10, color='red', label='HeterogeneousEnsemble')\n",
    "ax2.legend(loc='lower right')\n",
    "ax2.set_title(\"N=10\")\n",
    "ax2.set_xticks(list(range(0, len(models))))\n",
    "ax2.set_xticklabels([m for m in models])\n",
    "ax2.set_xlabel(\"Accuracy\")\n",
    "ax2.set_ylabel(\"Stack estimator\")\n",
    "\n",
    "f.suptitle(\"Accuracy of Varying Classifiers for Stack\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are seeing pretty similar behaviour for n=10 and n=30. In all cases the stackedHeterogeneousEnsemble is underperforming, but performing best when using with logistic regression as the stack estimator. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am curious to see if varying the data ratio to the stack will effect performance. We will test a range of values, while keeping n=10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = []\n",
    "ratio = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "for r in ratio:\n",
    "    print(r)\n",
    "    clf = StackedHeterogenousEnsembleClassifier(base_estimator=svm.SVC(random_state=42), \n",
    "                                                hp_range_map=hparams['SVM'], n_estimators=10, \n",
    "                                                random_state=0, stack_layer_estimator=copy(models['LR']), \n",
    "                                                base_stack_data_ratio = r)\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_valid)\n",
    "    acc.append(accuracy_score(y_pred, y_valid))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax1 = plt.subplots(1, 1, figsize=(10,5), sharex=True)\n",
    "x = list(range(0,len(ratio)))\n",
    "ax1.plot(x, acc, label=\"StackedHeterogeneousEnsemble\", color=\"orange\")\n",
    "ax1.legend(loc='lower right')\n",
    "ax1.set_title(\"Accuracy for varying base stack data ratio\")\n",
    "ax1.set_xticks(list(range(0, len(ratio))))\n",
    "ax1.set_xticklabels(ratio)\n",
    "ax1.set_xlabel(\"Accuracy\")\n",
    "ax1.set_ylabel(\"base_stack_data_ratio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two peaks here, for ratio = 0.5, and ratio = 0.9. One imagines that a ratio of 0.9 would surely lead to underfitting of the stack estimator, I will cross validate to see if this holds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reps, folds = 5, 5\n",
    "acc1,acc2, acc3 = [], [], []\n",
    "for i in range (0, reps):\n",
    "    kf = KFold(n_splits = folds, shuffle = True)\n",
    "    clf1 = StackedHeterogenousEnsembleClassifier(base_estimator=svm.SVC(random_state=42), \n",
    "                                                hp_range_map=hparams['SVM'], n_estimators=10, \n",
    "                                                random_state=0, stack_layer_estimator=copy(models['LR']), \n",
    "                                                base_stack_data_ratio = .5)\n",
    "    clf2 = StackedHeterogenousEnsembleClassifier(base_estimator=svm.SVC(random_state=42), \n",
    "                                                hp_range_map=hparams['SVM'], n_estimators=10, \n",
    "                                                random_state=0, stack_layer_estimator=copy(models['LR']), \n",
    "                                                base_stack_data_ratio = .7)\n",
    "    clf3 = StackedHeterogenousEnsembleClassifier(base_estimator=svm.SVC(random_state=42), \n",
    "                                                hp_range_map=hparams['SVM'], n_estimators=10, \n",
    "                                                random_state=0, stack_layer_estimator=copy(models['LR']), \n",
    "                                                base_stack_data_ratio = .9)\n",
    "    print(\"rep \", i+1)\n",
    "    acc1.append(cross_val_score(clf1, X_train, y_train, cv=kf, scoring='accuracy', error_score=\"raise\").mean())\n",
    "    acc2.append(cross_val_score(clf2, X_train, y_train, cv=kf, scoring='accuracy', error_score=\"raise\").mean())\n",
    "    acc3.append(cross_val_score(clf3, X_train, y_train, cv=kf, scoring='accuracy', error_score=\"raise\").mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f, ax1 = plt.subplots(1, 1, figsize=(10,5), sharex=True)\n",
    "x = list(range(1,folds+1))\n",
    "ax1.plot(x, acc1, label=\"ratio=0.5\", color=\"orange\")\n",
    "ax1.plot(x, acc2, label=\"ratio=0.7\", color=\"blue\")\n",
    "ax1.plot(x, acc3, label=\"ratio=0.9\", color=\"green\")\n",
    "ax1.legend(loc='lower right')\n",
    "ax1.set_title(\"Cross validated accuracy for ratio = .5, .7 and .9\")\n",
    "ax1.set_xticks(list(range(1, folds+1)))\n",
    "ax1.set_xlabel(\"Accuracy\")\n",
    "ax1.set_ylabel(\"Folds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Interestingly, we can see that a ratio of .9 is consistently providing the highest accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the knowledge garnered from the previous tests, I will now check performance on the test set. \n",
    "- For both the heterogeneous ensemble and the stacked ensemble I will use SVM as a base classifier.\n",
    "- As accuracy seemed to increase as n increased for the heterogeneous ensemble, whereas it seemed to stagnate after n=20 for the stacked ensemble, I will set n=100 and n=20 respectively.\n",
    "- For the stacked ensemble, I will set the data ratio to 0.9 and use logistic regression as the estimator for the stacked layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1 = StackedHeterogenousEnsembleClassifier(base_estimator=svm.SVC(random_state=42), \n",
    "                                                hp_range_map=hparams['SVM'], n_estimators=10, \n",
    "                                                random_state=0, stack_layer_estimator=copy(models['LR']), \n",
    "                                                base_stack_data_ratio = .9)\n",
    "clf2 = HeterogenousEnsembleClassifier(base_estimator=svm.SVC(random_state=42), \n",
    "                                                hp_range_map=hparams['SVM'], n_estimators=100, \n",
    "                                                random_state=0)\n",
    "clf3 = svm.SVC(random_state=i+i)\n",
    "clf4 = BaggingClassifier(base_estimator=svm.SVC(random_state=i+i), random_state=i+i)\n",
    "clfs = [clf1,clf2,clf3,clf4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for c in clfs:\n",
    "    print(c.__class__.__name__)\n",
    "    c.fit(X_train, y_train)\n",
    "    results.append(accuracy_score(c.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.arange(1)\n",
    "plt.bar(X + 0.00, results[0], color = 'b', alpha=0.5, width = 0.05)\n",
    "plt.bar(X + 0.10, results[1], color = 'y', alpha=0.5, width = 0.05)\n",
    "plt.bar(X + 0.20, results[2], color = 'g', alpha=0.5, width = 0.05)\n",
    "plt.bar(X + 0.30, results[3], color = 'r', alpha=0.5, width = 0.05)\n",
    "plt.title(\"Accuracy on Test Data\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xticks(ticks=[0,0.1,0.2,0.3],\n",
    "           rotation=90,\n",
    "           labels=[\"StackedHeterogeneousEnsembleClassifier\",\"HeterogeneousEnsembleClassifier\",\"Single Classifier\",\"Bagged Classifier\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifiers have behaved just as we saw with the crossvalidated validation set. The Heterogeneous classifier has performed best, \n",
    "the stacked classifier is second, and the bagged and single classifier come in last."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Evaluation Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- First of all evaluate best classifier on the data with non stacked model\n",
    "\n",
    "-- Then use the same base classifier with varying different stacked estimators\n",
    "\n",
    "-- Then compare against "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Reflect on the Performance of the Different Models Evaluated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Write your refelcection here (max 300 words)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
